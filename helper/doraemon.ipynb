{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a4bfdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T03:46:23.608010Z",
     "iopub.status.busy": "2025-09-09T03:46:23.607257Z",
     "iopub.status.idle": "2025-09-09T03:46:28.910805Z",
     "shell.execute_reply": "2025-09-09T03:46:28.909628Z"
    },
    "papermill": {
     "duration": 5.308774,
     "end_time": "2025-09-09T03:46:28.912554",
     "exception": false,
     "start_time": "2025-09-09T03:46:23.603780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a12468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T03:46:28.919762Z",
     "iopub.status.busy": "2025-09-09T03:46:28.919414Z",
     "iopub.status.idle": "2025-09-09T03:46:32.677392Z",
     "shell.execute_reply": "2025-09-09T03:46:32.676561Z"
    },
    "papermill": {
     "duration": 3.763948,
     "end_time": "2025-09-09T03:46:32.679025",
     "exception": false,
     "start_time": "2025-09-09T03:46:28.915077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Author: -\n",
    "# 2025-06-01\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from logging import FileHandler, StreamHandler, Logger\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI, OpenAI, AsyncAzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "\n",
    "class RawData(BaseModel):\n",
    "    query: str\n",
    "    prompt: List[Dict[str, str]]\n",
    "    context: Optional[str] = None\n",
    "    g_t: Any  # ground_truth\n",
    "    t_p: Optional[float]=1.0\n",
    "\n",
    "class RP(BaseModel):\n",
    "    query: str\n",
    "    context: Optional[str] = None\n",
    "    g_t: Any  # ground_truth\n",
    "    r_s: Optional[str] = None\n",
    "    answer: Any\n",
    "    temperature: float = 1.0\n",
    "    tokens: float = 0\n",
    "    score: Optional[Any] = None\n",
    "\n",
    "class Doraemon:\n",
    "    \"\"\"\n",
    "    Author: -\n",
    "    Date: 2025-06-01\n",
    "    Unified client for AzureOpenAI and OpenAI.\n",
    "    \"\"\"\n",
    "    _client: Any = None\n",
    "    _async_client: Any = None\n",
    "    _provider: str = 'gpt3'  # or 'llama3'\n",
    "\n",
    "    # Cache for secrets\n",
    "    _secrets_cache: Dict[str, str] = {\n",
    "        # Kaggle secret web service is not good performance for large skill tasks.\n",
    "        \"KSUG_AZURE_ENDPOINT_URL\":\"\",\n",
    "        \"KSUG_AZURE_OPENAI_API_KEY\":\"\",\n",
    "        \"KSUG_AZURE_API_VERSION\":\"\",\n",
    "        \"KSUG_GPT35_TURBO\":\"\",\n",
    "        \"AZURE_ENDPOINT\": \"\",\n",
    "        \"AZURE_API_KEY\": \"\",\n",
    "        \"AZURE_API_VERSION\": \"\",\n",
    "        \"LLAMA3\": \"\",\n",
    "        \"MINISTRAL3B_ENDPOINT\": \"\",\n",
    "        \"MINISTRAL3B_KEY\": \"\",\n",
    "        \"MINISTRAL3B_API_VERSION\": \"\",\n",
    "        \"MINISTRAL3B_MODEL\": \"\",\n",
    "    }\n",
    "\n",
    "    _user_secrets = None  # class-level singleton for UserSecretsClient\n",
    "\n",
    "    @classmethod\n",
    "    def set_provider(cls, provider: str):\n",
    "        provider = provider.lower()\n",
    "        cls._provider = provider\n",
    "        cls._client = None  # Reset client to force re-init\n",
    "        cls._async_client = None\n",
    "\n",
    "    @classmethod\n",
    "    def _get_secret(cls, secret_label: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve secret only once and cache it.\n",
    "        Uses singleton-style UserSecretsClient.\n",
    "        \"\"\"\n",
    "        if cls._user_secrets is None:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            cls._user_secrets = UserSecretsClient()\n",
    "        try:\n",
    "            if secret_label not in cls._secrets_cache:\n",
    "                cls._secrets_cache[secret_label] = cls._user_secrets.get_secret(secret_label)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return cls._secrets_cache[secret_label]\n",
    "\n",
    "    @classmethod\n",
    "    def _initialize_client(cls, logger: Logger) -> Any:\n",
    "        if cls._client is not None:\n",
    "            return cls._client\n",
    "        if cls._provider == 'gpt3':\n",
    "            cls._client = AzureOpenAI(\n",
    "                azure_endpoint=cls._get_secret(\"KSUG_AZURE_ENDPOINT_URL\"),\n",
    "                api_key=cls._get_secret(\"KSUG_AZURE_OPENAI_API_KEY\"),\n",
    "                api_version=cls._get_secret(\"KSUG_AZURE_API_VERSION\")\n",
    "            )\n",
    "        elif cls._provider == 'llama3':\n",
    "            cls._client = AzureOpenAI(\n",
    "                azure_endpoint=cls._get_secret(\"AZURE_ENDPOINT\"),\n",
    "                api_key=cls._get_secret(\"AZURE_API_KEY\"),\n",
    "                api_version=cls._get_secret(\"AZURE_API_VERSION\")                \n",
    "            )\n",
    "        elif cls._provider == 'ministral3b':\n",
    "            from azure.ai.inference import ChatCompletionsClient\n",
    "            from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "            cls._client = ChatCompletionsClient(\n",
    "                endpoint=cls._get_secret(\"MINISTRAL3B_ENDPOINT\"),\n",
    "                credential=AzureKeyCredential(cls._get_secret(\"MINISTRAL3B_KEY\")),\n",
    "                api_version=cls._get_secret(\"MINISTRAL3B_API_VERSION\")\n",
    "            )\n",
    "        elif cls._provider == 'cloud_flare':\n",
    "            cls._client = OpenAI(\n",
    "                api_key=cls._get_secret(\"WORKERS_AI_API_TOKEN\"),\n",
    "                base_url=cls._get_secret(\"CLOUD_FLARE_CLIENT_URL\")\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\"Provider must be set to either 'azure' or 'cloud_flare'\")\n",
    "        return cls._client\n",
    "\n",
    "    @classmethod\n",
    "    def _initialize_aync_client(cls, logger=None) -> Any:\n",
    "        \"\"\"\n",
    "        Initialize and return the async client for the configured provider/model.\n",
    "        Sets cls._client if not already initialized.\n",
    "        \"\"\"\n",
    "        if cls._async_client is not None:\n",
    "            return cls._async_client\n",
    "    \n",
    "        if cls._provider == 'gpt3':\n",
    "            from openai import AsyncAzureOpenAI\n",
    "            cls._async_client = AsyncAzureOpenAI(\n",
    "                azure_endpoint=cls._get_secret(\"KSUG_AZURE_ENDPOINT_URL\"),\n",
    "                api_key=cls._get_secret(\"KSUG_AZURE_OPENAI_API_KEY\"),\n",
    "                api_version=cls._get_secret(\"KSUG_AZURE_API_VERSION\")\n",
    "            )\n",
    "        elif cls._provider == 'llama3':\n",
    "            from openai import AsyncAzureOpenAI\n",
    "            cls._async_client = AsyncAzureOpenAI(\n",
    "                azure_endpoint=cls._get_secret(\"AZURE_ENDPOINT\"),\n",
    "                api_key=cls._get_secret(\"AZURE_API_KEY\"),\n",
    "                api_version=cls._get_secret(\"AZURE_API_VERSION\")\n",
    "            )\n",
    "        elif cls._provider == 'cloud_flare':\n",
    "            from openai import AsyncOpenAI\n",
    "            cls._async_client = AsyncOpenAI(\n",
    "                api_key=cls._get_secret(\"WORKERS_AI_API_TOKEN\"),\n",
    "                base_url=cls._get_secret(\"CLOUD_FLARE_CLIENT_URL\")\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\"Provider must be set to a known value\")\n",
    "    \n",
    "        if logger:\n",
    "            logger.info(f\"Initialized client for provider {cls._provider}\")\n",
    "        return cls._async_client\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def inference(\n",
    "        cls,\n",
    "        logger: Logger,\n",
    "        messages: List[Dict] = [{\"role\": \"system\", \"content\": 'You are a helpful AI assistant'}, {\"role\": \"user\", \"content\": 'hi'}],\n",
    "        model: str = None,\n",
    "        temperature: float = 1.0,\n",
    "        max_tokens: int = 512,\n",
    "        top_p: float = 1,\n",
    "        frequency_penalty: float = 0.0,\n",
    "        presence_penalty: float = 0.0,\n",
    "        stop: str = None,\n",
    "        stream: bool = False\n",
    "    ) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        client = cls._initialize_client(logger)\n",
    "\n",
    "        if cls._provider == 'ministral3b':\n",
    "            use_model = model or cls._get_secret(\"MINISTRAL3B_MODEL\")\n",
    "            response = client.complete(\n",
    "                stream=stream,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                model=use_model\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            tokens = getattr(response.usage, 'completion_tokens', 0)\n",
    "            return content, tokens\n",
    "\n",
    "        if cls._provider == 'gpt3':\n",
    "            deployment = cls._get_secret(\"KSUG_GPT35_TURBO\")\n",
    "        elif cls._provider == 'llama3':\n",
    "            deployment = cls._get_secret(\"LLAMA3\")\n",
    "        else:\n",
    "            deployment = model  # Model name should be explicit for OpenAI\n",
    "            if deployment is None:\n",
    "                raise ValueError(\"Model name must be provided.\")\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            stop=stop,\n",
    "            stream=stream\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        tokens = response.usage.completion_tokens\n",
    "        return content, tokens\n",
    "\n",
    "    @classmethod\n",
    "    def token_log_prob(\n",
    "        cls,\n",
    "        logger: Logger,\n",
    "        messages: List[Dict] = [{\"role\": \"system\", \"content\": 'You are a helpful AI assistant'}, {\"role\": \"user\", \"content\": 'hi'}],\n",
    "        model: str = None,\n",
    "        temperature: float = 1.0,\n",
    "        max_tokens: int = 512,\n",
    "        top_p: float = 0.9,\n",
    "        frequency_penalty: float = 0.0,\n",
    "        presence_penalty: float = 0.0,\n",
    "        stop: str = None,\n",
    "        stream: bool = False,\n",
    "        logprobs: bool = True\n",
    "    ) -> Tuple[Dict[str, int], List[Dict[str, float]]]:\n",
    "        \"\"\"\n",
    "        Get output tokens and logprobs for each generated token.\n",
    "        Returns:\n",
    "            - Dict[content, tokens_used]\n",
    "            - List[{'token': ..., 'logprob': ...}]\n",
    "        \"\"\"\n",
    "        client = cls._initialize_client(logger)\n",
    "        \n",
    "        if cls._provider == 'gpt3':\n",
    "            deployment = cls._get_secret(\"KSUG_GPT35_TURBO\")\n",
    "        elif cls._provider == 'llama3':\n",
    "            deployment = cls._get_secret(\"LLAMA3\")\n",
    "        else:\n",
    "            deployment = model\n",
    "            if deployment is None:\n",
    "                raise ValueError(\"Model name must be provided.\")\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            stop=stop,\n",
    "            stream=stream,\n",
    "            logprobs=logprobs\n",
    "        )\n",
    "    \n",
    "        choice = response.choices[0]\n",
    "        content = choice.message.content\n",
    "        tokens = response.usage.completion_tokens\n",
    "    \n",
    "        token_info = []\n",
    "        # Defensive: check logprobs exists and content is not None\n",
    "        if hasattr(choice, 'logprobs') and getattr(choice.logprobs, 'content', None) is not None:\n",
    "            for t in choice.logprobs.content:\n",
    "                token_info.append({\n",
    "                    \"token\": t.token,\n",
    "                    \"logprob\": float(getattr(t, \"logprob\", float('nan')))\n",
    "                })\n",
    "    \n",
    "        return {content: tokens}, token_info\n",
    "\n",
    "    @classmethod\n",
    "    def get_logger(\n",
    "        cls,\n",
    "        name: str = __name__,\n",
    "        level: int = logging.INFO,\n",
    "        fmt: str = \"%(asctime)s %(levelname)s %(message)s\",\n",
    "        logfile: str = \"doraemon.log\",\n",
    "        file_mode: str = \"a\"\n",
    "    ) -> Logger:\n",
    "        logger = logging.getLogger(name)\n",
    "        logger.setLevel(level)\n",
    "        if not logger.handlers:\n",
    "            console_h = StreamHandler()\n",
    "            console_h.setFormatter(logging.Formatter(fmt))\n",
    "            console_h.setLevel(level)\n",
    "            logger.addHandler(console_h)\n",
    "            file_h = FileHandler(logfile, mode=file_mode, encoding=\"utf-8\", delay=False)\n",
    "            file_h.setFormatter(logging.Formatter(fmt))\n",
    "            file_h.setLevel(level)\n",
    "            logger.addHandler(file_h)\n",
    "        return logger\n",
    "\n",
    "    @classmethod\n",
    "    def get_answer(cls, raw_answer: str) -> str:\n",
    "        match = re.search(r'\\\\boxed{([^}]+)}', raw_answer, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        text = str(raw_answer).strip()\n",
    "        return text\n",
    "\n",
    "    @classmethod\n",
    "    def process_data(cls, args: Dict[str, Any], logger:Logger) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Processes a single data item using Doraemon inference and builds the result.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            r_s, tokens = cls.inference(\n",
    "                logger=logger,\n",
    "                messages=args.get('prompt'),\n",
    "                temperature=args.get('t_p')\n",
    "            )\n",
    "            result = RP(\n",
    "                query=args.get('query'),\n",
    "                context=args.get('context'),\n",
    "                r_s=r_s,\n",
    "                answer=cls.get_answer(r_s),\n",
    "                g_t=args.get('g_t'),\n",
    "                temperature=float(args.get('t_p')),\n",
    "                tokens=int(tokens),\n",
    "                score=0.0\n",
    "            )\n",
    "            return result.model_dump()\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error processing question {args.get('query')} at temperature {args.get('t_p')} with exception {e}\")\n",
    "            return None\n",
    "\n",
    "    @classmethod\n",
    "    def run_parallel_inference(cls, tasks: List[Dict[str, Any]], logger:Logger, max_workers: int = 1) -> List[Optional[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Runs process_data in parallel over a list of tasks using ProcessPoolExecutor.\n",
    "        \"\"\"\n",
    "        process_func = partial(cls.process_data, logger=logger)\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_func, tasks), total=len(tasks)))\n",
    "        return results\n",
    "\n",
    "    @classmethod\n",
    "    def build_tasks(cls, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Convert expanded DataFrame to a list of model-valid dicts for RawData.\n",
    "        \"\"\"\n",
    "        return [RawData.model_validate(item).model_dump() for item in df.to_dict(orient='records')]\n",
    "\n",
    "    @classmethod\n",
    "    async def async_inference(\n",
    "        cls,\n",
    "        logger: Logger,\n",
    "        prompts: List[List[Dict]],\n",
    "        temperatures: List[float] = None,\n",
    "        max_tokens: int = 512,\n",
    "        top_p: float = 1.0,\n",
    "        frequency_penalty: float = 0.0,\n",
    "        presence_penalty: float = 0.0,\n",
    "        stop: str = None,\n",
    "        stream: bool = False,\n",
    "        max_concurrent: int = 20,\n",
    "        max_retries: int = 5,  # Set a max retry limit\n",
    "        backoff_factor: float = 2  # Exponential backoff factor\n",
    "    ) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Concurrent async inference using AsyncAzureOpenAI with retry logic.\n",
    "        Each prompt is a list of messages (List[Dict]).\n",
    "        Accepts a temperatures list, one per prompt.\n",
    "        \"\"\"\n",
    "        client = cls._initialize_aync_client(logger)\n",
    "        if cls._provider == 'gpt3':\n",
    "            deployment = cls._get_secret(\"KSUG_GPT35_TURBO\")\n",
    "        elif cls._provider == 'llama3':\n",
    "            deployment = cls._get_secret(\"LLAMA3\")\n",
    "        else:\n",
    "            deployment = model  # Model name should be explicit for OpenAI\n",
    "            if deployment is None:\n",
    "                raise ValueError(\"Model name must be provided.\")\n",
    "        \n",
    "        semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "        n = len(prompts)\n",
    "        results: List[Tuple[str, int]] = [(\"\", 0)] * n\n",
    "    \n",
    "        if temperatures is None:\n",
    "            temperatures = [1.0] * n\n",
    "    \n",
    "        async def infer_single(idx: int, messages: List[Dict], temp: float) -> None:\n",
    "            async with semaphore:\n",
    "                retries = 0\n",
    "                while retries < max_retries:\n",
    "                    try:\n",
    "                        resp = await client.chat.completions.create(\n",
    "                            model=deployment,\n",
    "                            messages=messages,\n",
    "                            max_tokens=max_tokens,\n",
    "                            temperature=temp,\n",
    "                            top_p=top_p,\n",
    "                            frequency_penalty=frequency_penalty,\n",
    "                            presence_penalty=presence_penalty,\n",
    "                            stop=stop,\n",
    "                            stream=stream\n",
    "                        )\n",
    "                        content = resp.choices[0].message.content\n",
    "                        tokens = getattr(resp.usage, \"completion_tokens\", 0)\n",
    "                        results[idx] = (content, tokens)\n",
    "                        break  # Exit retry loop on success\n",
    "                    except Exception as e:\n",
    "                        if \"429\" in str(e):  # Check if error is due to rate limit\n",
    "                            retries += 1\n",
    "                            backoff_time = backoff_factor ** retries\n",
    "                            logger.error(f\"Inference failed for index {idx}, retrying in {backoff_time}s: {e}\")\n",
    "                            await asyncio.sleep(backoff_time)  # Wait before retrying\n",
    "                        else:\n",
    "                            logger.error(f\"Inference failed for index {idx}: {e}\")\n",
    "                            results[idx] = (None, 0)\n",
    "                            break  # Exit retry loop on non-429 error\n",
    "    \n",
    "        tasks = [infer_single(i, msg, temperatures[i]) for i, msg in enumerate(prompts)]\n",
    "        \n",
    "        await asyncio.gather(*tasks)\n",
    "        return results\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    async def run_parallel_inference_async(\n",
    "        cls,\n",
    "        tasks: List[Dict[str, Any]],\n",
    "        logger: Logger,\n",
    "        max_concurrent: int = 50,\n",
    "        max_tokens: int = 512,\n",
    "        top_p: float = 1.0,\n",
    "        frequency_penalty: float = 0.0,\n",
    "        presence_penalty: float = 0.0,\n",
    "        stop: str = None,\n",
    "        stream: bool = False,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Run async_inference over all tasks in parallel (batch async inference).\n",
    "        Each task must have a 'prompt' key containing a List[Dict],\n",
    "        and can have a 'temperature' key.\n",
    "        Returns a list of model_dump() results from RP class, one per task.\n",
    "        \"\"\"\n",
    "        prompts = [task['prompt'] for task in tasks]\n",
    "        temperatures = [task.get('temperature', 1.0) for task in tasks]\n",
    "        results = await cls.async_inference(\n",
    "            logger=logger,\n",
    "            prompts=prompts,\n",
    "            temperatures=temperatures,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            stop=stop,\n",
    "            stream=stream,\n",
    "            max_concurrent=max_concurrent,\n",
    "        )\n",
    "\n",
    "        output = []\n",
    "        for task, (r_s, tokens) in zip(tasks, results):\n",
    "            result = RP(\n",
    "                query=task.get('query'),\n",
    "                context=task.get('context'),\n",
    "                r_s=r_s,\n",
    "                answer=cls.get_answer(r_s),\n",
    "                g_t=task.get('g_t'),\n",
    "                temperature=float(task.get('temperature', 1.0)),\n",
    "                tokens=int(tokens),\n",
    "                score=0.0\n",
    "            )\n",
    "            output.append(result.model_dump())\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.246726,
   "end_time": "2025-09-09T03:46:33.400492",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-09T03:46:19.153766",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
