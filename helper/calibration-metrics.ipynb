{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50644d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T06:06:41.989681Z",
     "iopub.status.busy": "2025-09-08T06:06:41.989261Z",
     "iopub.status.idle": "2025-09-08T06:07:08.006916Z",
     "shell.execute_reply": "2025-09-08T06:07:08.005752Z"
    },
    "papermill": {
     "duration": 26.023467,
     "end_time": "2025-09-08T06:07:08.008974",
     "exception": false,
     "start_time": "2025-09-08T06:06:41.985507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from torchmetrics.classification import BinaryCalibrationError\n",
    "\n",
    "\n",
    "class CalibrationMetrics:\n",
    "    \"\"\"\n",
    "    Metric utilities for confidence calibration & selective prediction.\n",
    "    Pass the column names you want (prob_col, correct_col) to every method.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    @classmethod\n",
    "    def _valid_mask(cls, df: pd.DataFrame, prob_col: str, correct_col: str) -> np.ndarray:\n",
    "        \"\"\"Valid rows have finite prob and finite correctness.\"\"\"\n",
    "        return np.isfinite(pd.to_numeric(df[prob_col], errors=\"coerce\")) & \\\n",
    "               np.isfinite(pd.to_numeric(df[correct_col], errors=\"coerce\"))\n",
    "\n",
    "    @classmethod\n",
    "    def _y_true_prob(cls, df: pd.DataFrame, prob_col: str, correct_col: str):\n",
    "        \"\"\"Return (y_true, y_prob) arrays filtered to valid rows with prob clipped to [0,1].\"\"\"\n",
    "        m = cls._valid_mask(df, prob_col, correct_col)\n",
    "        y_true = df.loc[m, correct_col].astype(int).to_numpy()\n",
    "        y_prob = pd.to_numeric(df.loc[m, prob_col], errors=\"coerce\").clip(0.0, 1.0).to_numpy()\n",
    "        return y_true, y_prob\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def normalized_selective_auc(cls, df: pd.DataFrame, prob_col: str, correct_col: str, anchor: bool = False):\n",
    "        \"\"\"\n",
    "        Normalized selective AUC:\n",
    "          - 0.0 = random ranking baseline\n",
    "          - 1.0 = perfect ranking (all correct before all incorrect)\n",
    "        Returns (normalized_auc, raw_auc, coverage, accuracy_curve).\n",
    "        If anchor=True, the curve is anchored at (coverage=0, accuracy=1); the random baseline area becomes (1+acc_full)/2.\n",
    "        \"\"\"\n",
    "        y_true, y_prob = cls._y_true_prob(df, prob_col, correct_col)\n",
    "        n = len(y_true)\n",
    "        if n == 0:\n",
    "            return math.nan, math.nan, np.array([]), np.array([])\n",
    "    \n",
    "        # Raw SAUC on current probs (trapz)\n",
    "        idx = np.argsort(-y_prob)\n",
    "        y_sorted = y_true[idx].astype(int)\n",
    "        coverage = np.arange(1, n + 1) / n\n",
    "        accuracy_curve = np.cumsum(y_sorted) / np.arange(1, n + 1)\n",
    "        if anchor:\n",
    "            cov_a = np.concatenate(([0.0], coverage))\n",
    "            acc_a = np.concatenate(([1.0], accuracy_curve))\n",
    "        else:\n",
    "            cov_a, acc_a = coverage, accuracy_curve\n",
    "        auc_raw = float(np.trapz(acc_a, cov_a))\n",
    "    \n",
    "        # Perfect-ranking area\n",
    "        y_perfect = np.sort(y_true)[::-1]\n",
    "        acc_p = np.cumsum(y_perfect) / np.arange(1, n + 1)\n",
    "        if anchor:\n",
    "            cov_p = np.concatenate(([0.0], coverage))\n",
    "            acc_p = np.concatenate(([1.0], acc_p))\n",
    "        else:\n",
    "            cov_p = coverage\n",
    "        auc_perfect = float(np.trapz(acc_p, cov_p))\n",
    "    \n",
    "        # Random baseline area\n",
    "        acc_full = float(np.mean(y_true))\n",
    "        auc_random = 0.5 * (1.0 + acc_full) if anchor else acc_full\n",
    "    \n",
    "        denom = (auc_perfect - auc_random)\n",
    "        n_auc = 0.0 if denom == 0.0 else (auc_raw - auc_random) / denom\n",
    "    \n",
    "        return float(n_auc), float(auc_raw), coverage, accuracy_curve\n",
    "\n",
    "    \n",
    "    # ---------- metrics ----------\n",
    "    @classmethod\n",
    "    def selective_auc(cls, df: pd.DataFrame, prob_col: str, correct_col: str):\n",
    "        \"\"\"\n",
    "        AUC of the selective accuracyâ€“coverage curve.\n",
    "        Returns (auc, coverage_array, accuracy_curve_array).\n",
    "        \"\"\"\n",
    "        y_true, y_prob = cls._y_true_prob(df, prob_col, correct_col)\n",
    "        if len(y_true) == 0:\n",
    "            return math.nan, np.array([]), np.array([])\n",
    "        idx = np.argsort(-y_prob)  # descending by confidence\n",
    "        y_sorted = y_true[idx]\n",
    "        n = len(y_sorted)\n",
    "        coverage = np.arange(1, n + 1) / n\n",
    "        accuracy_curve = np.cumsum(y_sorted) / np.arange(1, n + 1)\n",
    "        auc = np.trapz(accuracy_curve, coverage)\n",
    "        return float(auc), coverage, accuracy_curve\n",
    "\n",
    "    @classmethod\n",
    "    def ece_torchmetrics_binary(\n",
    "        cls, df: pd.DataFrame, prob_col: str, correct_col: str, n_bins: int = 10, norm: str = \"l2\"\n",
    "    ) -> float:\n",
    "        \"\"\"Expected Calibration Error via torchmetrics (norm='l2' squared, 'l1' absolute).\"\"\"\n",
    "        y_true, y_prob = cls._y_true_prob(df, prob_col, correct_col)\n",
    "        if len(y_true) == 0:\n",
    "            return math.nan\n",
    "        eps = 1e-12\n",
    "        y_prob_t = torch.tensor(np.clip(y_prob, eps, 1 - eps), dtype=torch.float32)\n",
    "        y_true_t = torch.tensor(y_true.astype(int), dtype=torch.long)\n",
    "        metric = BinaryCalibrationError(n_bins=n_bins, norm=norm)\n",
    "        return float(metric(y_prob_t, y_true_t))\n",
    "\n",
    "    @classmethod\n",
    "    def brier(cls, df: pd.DataFrame, prob_col: str, correct_col: str) -> float:\n",
    "        \"\"\"Brier score (MSE between correctness and confidence).\"\"\"\n",
    "        y_true, y_prob = cls._y_true_prob(df, prob_col, correct_col)\n",
    "        if len(y_true) == 0:\n",
    "            return math.nan\n",
    "        return float(brier_score_loss(y_true, y_prob))\n",
    "\n",
    "    @classmethod\n",
    "    def reliability_table(cls, df: pd.DataFrame, prob_col: str, correct_col: str, n_bins: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a reliability table using your VisuaCalibration helper.\n",
    "        Filters invalid rows before plotting/tabulating.\n",
    "        \"\"\"\n",
    "        m = cls._valid_mask(df, prob_col, correct_col)\n",
    "        if not m.any():\n",
    "            return pd.DataFrame(columns=[\"bin_lower\", \"bin_upper\", \"bin_center\", \"count\", \"accuracy\", \"confidence\"])\n",
    "\n",
    "    # ---------- one-call summary ----------\n",
    "    @classmethod\n",
    "    def summarize(\n",
    "        cls,\n",
    "        df: pd.DataFrame,\n",
    "        prob_col: str,\n",
    "        correct_col: str,\n",
    "        n_bins: int = 10,\n",
    "        norm: str = \"l2\",\n",
    "        include_curves: bool = True,\n",
    "        include_table: bool = True,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Compute a summary dict for arbitrary columns.\n",
    "        Keys: 'selective_auc', 'ece', 'brier', and optionally 'coverage', 'accuracy_curve', 'reliability_table'.\n",
    "        \"\"\"\n",
    "        auc, cov, acc = cls.selective_auc(df, prob_col=prob_col, correct_col=correct_col)\n",
    "        ece = cls.ece_torchmetrics_binary(df, prob_col=prob_col, correct_col=correct_col, n_bins=n_bins, norm=norm)\n",
    "        brier = cls.brier(df, prob_col=prob_col, correct_col=correct_col)\n",
    "        out = {\"selective_auc\": auc, \"ece\": ece, \"brier\": brier}\n",
    "        if include_curves:\n",
    "            out.update({\"coverage\": cov, \"accuracy_curve\": acc})\n",
    "        if include_table:\n",
    "            out[\"reliability_table\"] = cls.reliability_table(df, prob_col=prob_col, correct_col=correct_col, n_bins=n_bins)\n",
    "        return out\n",
    "\n",
    "    # ---------- optional: batch over many (prob, correct) pairs ----------\n",
    "    @classmethod\n",
    "    def summarize_many(cls, df: pd.DataFrame, pairs: list[tuple[str, str]], n_bins: int = 10, norm: str = \"l2\") -> dict:\n",
    "        \"\"\"\n",
    "        Compute summaries for multiple (prob_col, correct_col) pairs.\n",
    "        Returns a dict keyed by '<prob_col>/<correct_col>' -> summary dict.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for prob_col, correct_col in pairs:\n",
    "            key = f\"{prob_col}/{correct_col}\"\n",
    "            results[key] = cls.summarize(df, prob_col, correct_col, n_bins=n_bins, norm=norm, include_curves=False, include_table=False)\n",
    "        return results\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35.255474,
   "end_time": "2025-09-08T06:07:11.130985",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-08T06:06:35.875511",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
