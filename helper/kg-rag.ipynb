{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd35f14",
   "metadata": {
    "_cell_guid": "0ff862d2-7520-4424-b888-9387b4439327",
    "_uuid": "fc0d7aa6-5713-48e1-a72c-cfb9cf5d5f29",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-04T07:51:04.694916Z",
     "iopub.status.busy": "2025-09-04T07:51:04.694597Z",
     "iopub.status.idle": "2025-09-04T07:51:15.150630Z",
     "shell.execute_reply": "2025-09-04T07:51:15.149322Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 10.46334,
     "end_time": "2025-09-04T07:51:15.152609",
     "exception": false,
     "start_time": "2025-09-04T07:51:04.689269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import asyncio\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "\n",
    "\n",
    "from doraemon import Doraemon\n",
    "\n",
    "BOXED_PAT_DOUBLE = re.compile(r'\\\\boxed\\s*\\{\\s*\\{(.*?)\\}\\s*\\}')\n",
    "BOXED_PAT_SINGLE = re.compile(r'\\\\boxed\\s*\\{\\s*(.*?)\\s*\\}')\n",
    "\n",
    "# New regex patterns for extracting answer + confidence\n",
    "BOXED_CONF_DOUBLE = re.compile(r\"\\\\boxed\\{\\{([^,{}]+)\\s*,\\s*([0-9]*\\.?[0-9]+)\\}\\}\")\n",
    "BOXED_CONF_SINGLE = re.compile(r\"\\\\boxed\\{([^,{}]+)\\s*,\\s*([0-9]*\\.?[0-9]+)\\}\")\n",
    "\n",
    "class KG_RAG_Tool:\n",
    "    \"\"\"\n",
    "    A utility class containing common helper methods used throughout the KG‑RAG\n",
    "    pipeline. All methods are defined as class methods so they can be\n",
    "    invoked without instantiating the class.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def extract_boxed_answer(cls, text):\n",
    "        \"\"\"\n",
    "        Extracts the answer string from a value like '(\\boxed{{David Hewlett}}, 7)'\n",
    "        or '(\\boxed{2009}, 6)'. Returns None if not found.\n",
    "        \"\"\"\n",
    "        if text is None or (isinstance(text, float) and pd.isna(text)):\n",
    "            return None\n",
    "        s = str(text)\n",
    "    \n",
    "        # Primary: find content inside \\boxed{{...}} or \\boxed{...}\n",
    "        m = BOXED_PAT_DOUBLE.search(s)\n",
    "        if not m:\n",
    "            m = BOXED_PAT_SINGLE.search(s)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "    \n",
    "        # Fallback: if something like '(answer, 7)' without boxed\n",
    "        # take the first comma-separated piece inside top-level parentheses\n",
    "        m2 = re.match(r'^\\s*\\(([^,]+),', s)\n",
    "        if m2:\n",
    "            return m2.group(1).strip()\n",
    "    \n",
    "        return None\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def extract_boxed_answer_and_confidence(\n",
    "        cls, text: Optional[str]\n",
    "    ) -> Optional[Tuple[Optional[str], Optional[float]]]:\n",
    "        \"\"\"\n",
    "        Extracts the final answer and confidence score from model outputs.\n",
    "\n",
    "        Expected formats:\n",
    "            '\\\\boxed{{David Hewlett, 0.92}}'\n",
    "            '\\\\boxed{2009, 1.0}'\n",
    "            '(David Hewlett, 0.92)'  <-- fallback when \\boxed is missing\n",
    "\n",
    "        Args:\n",
    "            text (Optional[str]): Raw LLM output containing boxed answer and confidence score.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Tuple[Optional[str], Optional[float]]]:\n",
    "                - answer (str): Extracted answer string, stripped of extra spaces.\n",
    "                - confidence (float): Extracted confidence score, converted to float.\n",
    "                - Returns None if neither answer nor confidence could be extracted.\n",
    "        \"\"\"\n",
    "        # Handle empty, None, or NaN cases\n",
    "        if text is None or (isinstance(text, float) and pd.isna(text)):\n",
    "            return None, math.nan\n",
    "\n",
    "        s = str(text).strip()\n",
    "        m = BOXED_CONF_DOUBLE.search(s)\n",
    "        if m:\n",
    "            return m.group(1).strip(), float(m.group(2))\n",
    "        m2 = re.match(r'^\\s*\\(([^,]+)\\s*,\\s*([0-9]*\\.?[0-9]+)\\)\\s*$', s)\n",
    "        if m2:\n",
    "            return m2.group(1).strip(), float(m2.group(2))\n",
    "        return None, math.nan\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def norm(cls, x):\n",
    "        \"\"\"\n",
    "        Normalise answers for robust comparison:\n",
    "        - remove backslashes\n",
    "        - strip quotes/brackets/whitespace\n",
    "        - lowercase\n",
    "        - collapse internal whitespace\n",
    "        \"\"\"\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return None\n",
    "        s = str(x)\n",
    "        s = s.replace('\\\\', '')\n",
    "        s = s.strip().strip('\\'\"[]{}()')\n",
    "        s = re.sub(r'\\s+', ' ', s).lower()\n",
    "    \n",
    "        return s\n",
    "\n",
    "    @classmethod\n",
    "    def parse_label_list(cls, label_val):\n",
    "        \"\"\"\n",
    "        Ensure Label becomes a list[str] of normalised items.\n",
    "        Accepts:\n",
    "          - already a list/tuple/set -> normalise each item\n",
    "          - string that looks like a list -> ast.literal_eval\n",
    "          - other strings like '[a, b]' -> best-effort split\n",
    "        \"\"\"\n",
    "        if label_val is None or (isinstance(label_val, float) and pd.isna(label_val)):\n",
    "            return []\n",
    "    \n",
    "        if isinstance(label_val, (list, tuple, set)):\n",
    "            return [cls.norm(x) for x in label_val]\n",
    "    \n",
    "        s = str(label_val).strip()\n",
    "        # Try literal eval if it looks like a Python literal list\n",
    "        if s.startswith('[') and s.endswith(']'):\n",
    "            try:\n",
    "                val = ast.literal_eval(s)\n",
    "                if isinstance(val, (list, tuple, set)):\n",
    "                    return [cls.norm(x) for x in val]\n",
    "            except Exception:\n",
    "                # fall through to regex split\n",
    "                pass\n",
    "    \n",
    "            # Best-effort split on commas inside [ ... ]\n",
    "            inner = s[1:-1].strip()\n",
    "            if not inner:\n",
    "                return []\n",
    "            parts = [p.strip() for p in inner.split(',')]\n",
    "            return [cls.norm(p) for p in parts]\n",
    "    \n",
    "        # Otherwise treat as a single label\n",
    "        return [cls.norm(s)]\n",
    "\n",
    "    @classmethod\n",
    "    def build_fusion_contents(cls, q, ctxs, answer):\n",
    "        \"\"\"\n",
    "        Build the concatenated contents for the fusion prompt. It concatenates\n",
    "        the question, multiple contexts and the prior answer into a single\n",
    "        string. Contexts may be None, a numpy array, a string or a list of\n",
    "        strings.\n",
    "        \"\"\"\n",
    "        if ctxs is None:\n",
    "            ctxs = []\n",
    "        if isinstance(ctxs, np.ndarray):\n",
    "            ctxs = ctxs.tolist()\n",
    "        if not isinstance(ctxs, (list, tuple)):\n",
    "            ctxs = [ctxs]\n",
    "        lines = [f\"Question: {q}\"] + [f\"Context{i+1}: {c}\" for i, c in enumerate(ctxs)] + [f\"Answer: {answer}\"]\n",
    "        return \"\".join(lines)\n",
    "\n",
    "    @classmethod\n",
    "    def compute_metrics(cls, df, answer_col=\"answerable\", decision_col=\"fusion\"):\n",
    "        \"\"\"\n",
    "        Compute the contingency counts and derived metrics. The metrics\n",
    "        returned match those defined in the original notebooks and include\n",
    "        Risk, Carefulness, Alignment and Coverage percentages.\n",
    "        \"\"\"\n",
    "        a = df[answer_col].astype(str).str.strip().str.upper()\n",
    "        d = df[decision_col].astype(str).str.strip().str.upper()\n",
    "        AK = ((a == \"A\") & (d == \"K\")).sum()\n",
    "        AD = ((a == \"A\") & (d == \"D\")).sum()\n",
    "        UK = ((a == \"U\") & (d == \"K\")).sum()\n",
    "        UD = ((a == \"U\") & (d == \"D\")).sum()\n",
    "        N = AK + AD + UK + UD\n",
    "        def pct(num, den):\n",
    "            return 100.0 * (float(num) / den) if den > 0 else np.nan\n",
    "        return pd.DataFrame([{\n",
    "            \"AK\": AK, \"AD\": AD, \"UK\": UK, \"UD\": UD, \"N\": N,\n",
    "            \"Risk %\": pct(UK, AK + UK),\n",
    "            \"Carefulness %\": pct(UD, UK + UD),\n",
    "            \"Alignment %\": pct(AK + UD, N),\n",
    "            \"Coverage %\": pct(AK + UK, N),\n",
    "        }])\n",
    "\n",
    "    @classmethod\n",
    "    def eval_accuracy(\n",
    "        cls,\n",
    "        df: pd.DataFrame,\n",
    "        pred: str,\n",
    "        g_t: str,\n",
    "        out_col: str = \"is_correct\"\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate accuracy by checking if prediction is inside the label list.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing predictions and labels.\n",
    "            pred (str): Column name for predictions.\n",
    "            g_t (str): Column name for ground-truth labels (list-like).\n",
    "            out_col (str): Column name to store correctness flags.\n",
    "\n",
    "        Returns:\n",
    "            dict: {\"accuracy\": float, \"num_correct\": int, \"total\": int}\n",
    "        \"\"\"\n",
    "\n",
    "        def _in_labels(p, labels) -> bool:\n",
    "            if p is None or (isinstance(p, float) and pd.isna(p)):\n",
    "                return False\n",
    "\n",
    "            if isinstance(labels, np.ndarray):\n",
    "                labels = labels.tolist()\n",
    "            if not isinstance(labels, (list, tuple, set)):\n",
    "                return False\n",
    "\n",
    "            norm_p =cls.norm(p)\n",
    "\n",
    "            norm_labels = {cls.norm(x) for x in labels if x is not None}\n",
    "\n",
    "            return norm_p in norm_labels\n",
    "\n",
    "        # Mark correctness per row\n",
    "        df[out_col] = df.apply(lambda r: _in_labels(r[pred], r[g_t]), axis=1)\n",
    "\n",
    "        total = len(df)\n",
    "        num_correct = int(df[out_col].sum()) if total else 0\n",
    "        accuracy = (num_correct / total) if total else float(\"nan\")\n",
    "\n",
    "        return {\"accuracy\": accuracy, \"num_correct\": num_correct, \"total\": total}\n",
    "        \n",
    "\n",
    "class PromptBuilder:\n",
    "    \"\"\"\n",
    "    A class encapsulating the prompt templates used throughout the KG‑RAG\n",
    "    pipeline. Two dictionaries are exposed: PROMPT for the initial RAG\n",
    "    prompting and PROMPT_FUSION for the fusion stage. Helper methods are\n",
    "    provided to assemble user messages.\n",
    "    \"\"\"\n",
    "    PROMPT = {\n",
    "        'rag': (\n",
    "            \"Use the provided contexts to answer the question. \"\n",
    "            \"Always return a confidence score between 0.00 and 1.00 reflecting how confident you are that the final answer is correct. \"\n",
    "            \"Output MUST be exactly one line in this format:\\n\"\n",
    "            \"\\\\boxed{{final answer, confidence score}}\\n\"\n",
    "            \"Do not include any other text. Examples:\\n\"\n",
    "            \"\\\\boxed{{David Hewlett, 0.92}}\\n\"\n",
    "        ),\n",
    "        'cf_use': (\n",
    "            \"Assume your previous answer is wrong due to improper use of the retrieved contexts. \"\n",
    "            \"Carefully re-check the provided contexts and regenerate single answer using one or a few words. \"\n",
    "            \"Always return a confidence score between 0.00 and 1.00 reflecting how confident you are that the final answer is correct. \"\n",
    "            \"Output MUST be exactly one line in this format:\\n\"\n",
    "            \"\\\\boxed{{final answer, confidence score}}\\n\"\n",
    "            \"Do not include any other text. Examples:\\n\"\n",
    "            \"\\\\boxed{{David Hewlett, 0.95}}\\n\"\n",
    "        ),\n",
    "        'cf_quality': (\n",
    "            \"Assume your previous answer is wrong because the quality of the referred contexts is poor. \"\n",
    "            \"Re-select the most relevant parts from the given contexts and regenerate single answer using one or a few words. \"\n",
    "            \"Always return a confidence score between 0.00 and 1.00 reflecting how confident you are that the final answer is correct. \"\n",
    "            \"Output MUST be exactly one line in this format:\\n\"\n",
    "            \"\\\\boxed{{final answer, confidence score}}\\n\"\n",
    "            \"Do not include any other text. Examples:\\n\"\n",
    "            \"\\\\boxed{{2009, 0.88}}\\n\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    PROMPT_FUSION = {\n",
    "        'fusion_use': (\n",
    "            \"Your answer is likely to be wrong because of the improper use of retrieval contexts. \"\n",
    "            \"Decide keep/discard AND provide the best final answer.\"\n",
    "            \"Do not include commas inside the final answer. If the answer has multiple items, \"\n",
    "            \"separate them with a semicolon and a space.\"\n",
    "            \"Output constraints:\"\n",
    "            \"- No extra text before or after the box.\"\n",
    "            \"- Exactly one box with exactly two fields: [K or D].\"\n",
    "            \"Return your decision and final answer in this exact format:\"\n",
    "            \"\\\\boxed{{K or D}}\"\n",
    "        ),\n",
    "        'fusion_qual': (\n",
    "            \"Your answer is likely to be wrong because of the poor quality of retrieval contexts. \"\n",
    "            \"Decide keep/discard AND provide the best final answer.\"\n",
    "            \"Do not include commas inside the final answer. If the answer has multiple items, \"\n",
    "            \"separate them with a semicolon and a space.\"\n",
    "            \"Output constraints:\"\n",
    "            \"- No extra text before or after the box.\"\n",
    "            \"- Exactly one box with exactly two fields: [K or D].\"\n",
    "            \"Return your decision and final answer in this exact format:\"\n",
    "            \"\\\\boxed{{K or D}}\"\n",
    "        ),# fixed the verbalized probability extremely high issue.\n",
    "        'fusion_prob': ( \n",
    "            \"Provide the probability that your regenerated answer is correct as a number between 0.00 to 1.00\"\n",
    "            \"Respond with ONLY the number (no words or symbols). \" \n",
    "            \"Return the answer in this exact format:\" \n",
    "            \"\\\\boxed{{[final answer]}}\" \n",
    "        ),\n",
    "    }\n",
    "\n",
    "    PROMPT_MULTIPLICATION = {\n",
    "        'cf_use': (\n",
    "            \"Assume the usage of the retrieved path in reasoning may be flawed. \"\n",
    "            \"Give the confidence score for the baseline answer, the best possible answer for the question, \"\n",
    "            \"and the confidence score for that answer based on the provided question and contexts. \"\n",
    "            \"Return your answer in the format: \\\\boxed{{baseline_confidence, answer, confidence_for_answer}}\\n\\n\"\n",
    "            \"Do not include any other text. Examples:\\n\"\n",
    "            \"\\\\boxed{{0.99, David, 0.88}}\\n\"\n",
    "        ),\n",
    "        'cf_quality': (\n",
    "            \"Assume the previous baseline answer may be wrong because the quality of the retrieved contexts is poor. \"\n",
    "            \"Give the confidence score for the baseline answer, the best possible answer for the question, \"\n",
    "            \"and the confidence score for that answer based on the provided question and contexts. \"\n",
    "            \"Return your answer in the format: \\\\boxed{{baseline_confidence, answer, confidence_for_answer}}\\n\\n\"\n",
    "            \"Do not include any other text. Examples:\\n\"\n",
    "            \"\\\\boxed{{0.99, David, 0.88}}\\n\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    VERB_PROMPTS = {\n",
    "        # ---------- Baseline RAG ----------\n",
    "        \"rag\": {\n",
    "            \"gen\": (\n",
    "                \"Provide your best guess for the following question and contexts. \"\n",
    "                \"Give ONLY the guess, no other words or explanation.\\n\\n\"\n",
    "                \"Return ONLY the guess in this EXACT format:\\n\"\n",
    "                \"\\\\boxed{guess}\\n\"\n",
    "            ),\n",
    "            \"score\": (\n",
    "                \"Provide the probability that your guess is correct. \"\n",
    "                \"Give ONLY the probability between 0.0 and 1.0, no other words or explanation.\\n\\n\"\n",
    "                \"Return ONLY the probability in this EXACT format:\\n\"\n",
    "                \"\\\\boxed{probability}\\n\"\n",
    "            ),\n",
    "        },\n",
    "        # ---------- Counterfactual: misuse of context ----------\n",
    "        \"cf_use\": {\n",
    "            \"gen\": (\n",
    "                \"Assume your previous guess may be wrong due to improper use of the retrieved contexts. \"\n",
    "                \"Carefully re-check the contexts and regenerate your best guess. \"\n",
    "                \"Answer in one or a few words; do not explain.\\n\\n\"\n",
    "                \"Return ONLY the guess in this EXACT format:\\n\"\n",
    "                \"\\\\boxed{guess}\\n\"\n",
    "            ),\n",
    "            \"score\": (\n",
    "                \"Given the question, contexts, and your new guess, provide the probability that guess is correct. \"\n",
    "                \"Return ONLY a number between 0.00 and 1.00; do not explain.\\n\\n\"\n",
    "                \"Return ONLY the probability in this EXACT format:\\n\"\n",
    "                \"\\\\boxed{probability}\\n\"\n",
    "            ),\n",
    "        },\n",
    "        # ---------- Counterfactual: low context quality ----------\n",
    "        \"cf_quality\": {\n",
    "            \"gen\": (\n",
    "                \"Assume your previous guess may be wrong because the quality of the contexts is poor. \"\n",
    "                \"Select the most relevant parts mentally and regenerate your best guess. \"\n",
    "                \"Answer in one or a few words; do not explain.\\n\\n\"\n",
    "                \"Return ONLY the guess in this EXACT format:\\n\"\n",
    "                \"\\\\boxed{guess}\\n\"\n",
    "            ),\n",
    "            \"score\": (\n",
    "                \"Given the question, contexts, and your new guess guess, provide the probability that guess is correct. \"\n",
    "                \"Return ONLY a number between 0.00 and 1.00; do not explain.\\n\\n\"\n",
    "                \"Return ONLY the probability in this EXACT format:\\n\"\n",
    "                \"\\\\boxed{probability}\\n\"\n",
    "            ),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def build_user_multi_contents(cls, q, ctxs):\n",
    "        if ctxs is None:\n",
    "            ctxs = []\n",
    "        if not isinstance(ctxs, (list, tuple)):\n",
    "            ctxs = [ctxs]\n",
    "        lines = [f\"Question: {q}\"] + [f\"\\nContext{i+1}: {c}\" for i, c in enumerate(ctxs)]\n",
    "        return \"\".join(lines)\n",
    "\n",
    "    @classmethod\n",
    "    def build_prob(cls, q, ctxs, answer, system_msg):\n",
    "        user_msg = (\n",
    "            f\"QUESTION:\\n{q}\\n\\n\"\n",
    "            f\"CONTEXT:\\n{ctxs}\\n\\n\"\n",
    "            \"YOUR GUESS:\\n\"\n",
    "            f\"{answer}\\n\\n\"\n",
    "            \"Return ONLY the probability less than 1.00 as: \\\\boxed{probability}\"\n",
    "        )\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def alter_answer(cls, q, ctxs, answer, system_msg):\n",
    "        user_msg = (\n",
    "            f\"QUESTION:\\n{q}\\n\\n\"\n",
    "            f\"CONTEXT:\\n{ctxs}\\n\\n\"\n",
    "            \"YOUR GUESS:\\n\"\n",
    "            f\"{answer}\\n\\n\"\n",
    "            \"Return ONLY the guess in this EXACT format:\\\\boxed{{guess}}\"\n",
    "            \"Example:\"\n",
    "            \"\\\\boxed{{Italian Language}}\"\n",
    "        )\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def causal_f(\n",
    "        cls,\n",
    "        candidates: str,          # e.g., \"a1,a2,a3,a4\"\n",
    "        q: str,\n",
    "        ctxs: str,\n",
    "        system_msg: str,\n",
    "        k: int = 3\n",
    "    ) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Build chat messages for Stage 2 (probability scoring).\n",
    "        `candidates` is a single comma-separated string like \"a1,a2,a3,a4\".\n",
    "        Output must be: \\boxed{p1,p2,...,pk}\n",
    "        \"\"\"\n",
    "        # Minimal safeguard: trim whitespace\n",
    "        cand_str = (candidates or \"\").strip()\n",
    "    \n",
    "        user_msg = (\n",
    "            f\"QUESTION:\\n{q}\\n\\n\"\n",
    "            f\"CONTEXT:\\n{ctxs}\\n\\n\"\n",
    "            \"YOUR GUESSES (comma-separated, in order):\\n\"\n",
    "            f\"{cand_str}\\n\\n\"\n",
    "            \"Example (inputs come from other prompts):\\n\"\n",
    "            \"G1: Masti\\n\"\n",
    "            \"G2: Masti Returns\\n\"\n",
    "            \"G3: masti\\n\"\n",
    "            \"Valid output:\\n\"\n",
    "            \"\\\\boxed{{{Masti:0.78, Masti Returns:0.22}}}\\n\"\n",
    "        )\n",
    "    \n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "\n",
    "\n",
    "class Inference:\n",
    "    \"\"\"\n",
    "    A class encapsulating the asynchronous batched inference logic. It uses\n",
    "    Doraemon's async_inference method to send batches of messages to the\n",
    "    underlying language model. The batch_size parameter controls how many\n",
    "    records are processed in each call.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    async def process_batches(cls, tasks, logger, column_name, batch_size=20):\n",
    "        results = []\n",
    "        for i in range(0, len(tasks), batch_size):\n",
    "            batch = tasks[i:i + batch_size]\n",
    "            batch_messages = [task[column_name] for task in batch]\n",
    "            r = await Doraemon.async_inference(\n",
    "                logger=logger,\n",
    "                prompts=batch_messages,\n",
    "                temperatures=[0.0] * len(batch)\n",
    "            )\n",
    "            results.extend(r)\n",
    "            await asyncio.sleep(5)\n",
    "        return results\n",
    "\n",
    "\n",
    "class Judgment:\n",
    "    \"\"\"\n",
    "    A class responsible for computing the various judgment flags and\n",
    "    intermediate answers. It operates directly on a DataFrame and\n",
    "    enriches it with new columns: answerable, cf_use_f, cf_quality_judge,\n",
    "    fusion, and final_a.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def apply_judgments(cls, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        - Extract answers from init_a, cf_use_a, cf_qual_a\n",
    "        - Normalise all extracted answers and Label(s)\n",
    "        - answerable: 'A' if init_a matches any label; else 'U'\n",
    "        - cf_use_f:  'K' if cf_use_a matches init_a;   else 'D'\n",
    "        - cf_quality_f: 'K' if cf_qual_a matches init_a; else 'D'\n",
    "        Returns the same dataframe with new columns added:\n",
    "          ['init_a_ans','cf_use_ans','cf_qual_ans','Label','answerable','cf_use_f','cf_quality_f']\n",
    "        \"\"\"\n",
    "        # Extract raw answers\n",
    "        df = df.copy()\n",
    "        # Apply extractor for init_a\n",
    "        df[[\"init_ans\", \"init_cs\"]] = df[\"init_a\"].apply(\n",
    "            lambda x: pd.Series(KG_RAG_Tool.extract_boxed_answer_and_confidence(x))\n",
    "        )\n",
    "        \n",
    "        # Apply extractor for cf_use_a\n",
    "        df[[\"cf_use_ans\", \"cf_use_cs\"]] = df[\"cf_use_a\"].apply(\n",
    "            lambda x: pd.Series(KG_RAG_Tool.extract_boxed_answer_and_confidence(x))\n",
    "        )\n",
    "        \n",
    "        # Apply extractor for cf_qual_a\n",
    "        df[[\"cf_qual_ans\", \"cf_qual_cs\"]] = df[\"cf_qual_a\"].apply(\n",
    "            lambda x: pd.Series(KG_RAG_Tool.extract_boxed_answer_and_confidence(x))\n",
    "        )\n",
    "\n",
    "    \n",
    "        # answerable: A if init_ans ∈ Label else U\n",
    "        def judge_answerable(row):\n",
    "            ia = row['init_ans']\n",
    "            labels = row['Label'] or []\n",
    "            return 'A' if ia is not None and ia in labels else 'U'\n",
    "    \n",
    "        # cf_use_f: K if cf_use_ans == init_ans else D\n",
    "        def judge_cf_use(row):\n",
    "            ia = row['init_ans']\n",
    "            ca = row['cf_use_ans']\n",
    "            if ia is None or ca is None:\n",
    "                return 'D'\n",
    "            return 'K' if ia == ca else 'D'\n",
    "    \n",
    "        # cf_quality_f: K if cf_qual_ans == init_ans else D\n",
    "        def judge_cf_quality(row):\n",
    "            ia = row['init_ans']\n",
    "            qa = row['cf_qual_ans']\n",
    "            if ia is None or qa is None:\n",
    "                return 'D'\n",
    "            return 'K' if ia == qa else 'D'\n",
    "    \n",
    "        df['answerable']   = df.apply(judge_answerable, axis=1)\n",
    "        df['cf_use_f']     = df.apply(judge_cf_use, axis=1)\n",
    "        df['cf_quality_f'] = df.apply(judge_cf_quality, axis=1)\n",
    "\n",
    "        def fusion_judge(row):\n",
    "            q, u = row['cf_use_f'], row['cf_quality_f']\n",
    "            if q == 'K' and u == 'K':\n",
    "                return 'K'\n",
    "            elif q == 'D' and u == 'D':\n",
    "                return 'D'\n",
    "            else:\n",
    "                return 'F'\n",
    "        \n",
    "        df['fusion'] = df.apply(fusion_judge, axis=1)\n",
    "\n",
    "        def select_final_answer(row):\n",
    "            # Case 1: Keep initial answer directly\n",
    "            if row['fusion'] == 'K':\n",
    "                return row['init_ans']\n",
    "            \n",
    "            # Case 2: Decide based on highest confidence score\n",
    "            if row['fusion'] == 'D':\n",
    "                # Prepare mapping between confidence scores and their answers\n",
    "                candidates = {\n",
    "                    row['init_cs']: row['init_ans'],\n",
    "                    row['cf_use_cs']: row['cf_use_ans'],\n",
    "                    row['cf_qual_cs']: row['cf_qual_ans']\n",
    "                }\n",
    "                \n",
    "                # Filter out None or NaN scores\n",
    "                valid_candidates = {cs: ans for cs, ans in candidates.items() if pd.notna(cs)}\n",
    "        \n",
    "                if not valid_candidates:\n",
    "                    return pd.NA  # If all scores are missing\n",
    "        \n",
    "                # Pick the answer with the highest confidence score\n",
    "                return valid_candidates[max(valid_candidates.keys())]\n",
    "        \n",
    "            # Case 3: No final answer for other fusion values\n",
    "            return pd.NA\n",
    "        \n",
    "        df['final_a'] = df.apply(select_final_answer, axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "class Fusion:\n",
    "    \"\"\"\n",
    "    A class encapsulating the fusion logic for resolving indeterminate ('F')\n",
    "    cases and for computing confidence scores. Methods in this class\n",
    "    construct the appropriate prompts, call the inference engine and update\n",
    "    the DataFrame with the resulting fusion decisions, final answers and\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    async def resolve_f_cases(cls, df: pd.DataFrame, logger, batch_size=20) -> pd.DataFrame:\n",
    "        df_f = df[df['fusion'] == 'F'].copy()\n",
    "        if df_f.empty:\n",
    "            return df\n",
    "        df_f['fusion_prompt'] = df_f.apply(\n",
    "            lambda row: [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": PromptBuilder.PROMPT_FUSION['fusion_use'] if row[\"cf_use_f\"] == \"D\" else PromptBuilder.PROMPT_FUSION['fusion_qual']\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": KG_RAG_Tool.build_fusion_contents(\n",
    "                        q=row[\"question\"],\n",
    "                        ctxs=row[\"ctx_topk\"],\n",
    "                        answer=row[\"cf_use_ans\"] if row[\"cf_use_f\"] == \"D\" else row[\"cf_qual_ans\"]\n",
    "                    )\n",
    "                }\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        tasks = df_f.to_dict(orient='records')\n",
    "        final_f = await Inference.process_batches(tasks, logger, 'fusion_prompt', batch_size=batch_size)\n",
    "        answers = [KG_RAG_Tool.extract_boxed_answer(x[0]) for x in final_f]\n",
    "        df_f[\"fusion\"] = [ans.split(\",\", 1)[0].strip().upper() if ans else None for ans in answers]\n",
    "        df_f[\"final_a\"] = [ans.split(\",\", 1)[1].strip() if ans and \",\" in ans else pd.NA for ans in answers]\n",
    "        df.loc[df_f.index, \"fusion\"] = df_f[\"fusion\"]\n",
    "        df.loc[df_f.index, \"final_a\"] = df_f[\"final_a\"]\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    async def compute_probabilities(cls, df: pd.DataFrame, logger, batch_size=20) -> pd.DataFrame:\n",
    "        df['prob_query'] = df.apply(\n",
    "            lambda row: [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": PromptBuilder.PROMPT_FUSION['fusion_prob']\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": KG_RAG_Tool.build_fusion_contents(\n",
    "                        q=row[\"question\"],\n",
    "                        ctxs=row[\"ctx_topk\"],\n",
    "                        answer=row[\"final_a\"]\n",
    "                    )\n",
    "                }\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        tasks = df.to_dict(orient='records')\n",
    "        fusion_prob = await Inference.process_batches(tasks, logger, 'prob_query', batch_size=batch_size)\n",
    "        df['fusion_prob'] = pd.Series(fusion_prob, dtype=\"object\")\n",
    "        df['fusion_prob'] = df['fusion_prob'].apply(KG_RAG_Tool.extract_boxed_answer)\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 250973825,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.347537,
   "end_time": "2025-09-04T07:51:15.978651",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-04T07:50:59.631114",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
