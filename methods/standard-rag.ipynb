{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12901053,"sourceType":"datasetVersion","datasetId":8162483},{"sourceId":12936708,"sourceType":"datasetVersion","datasetId":8185966},{"sourceId":250973825,"sourceType":"kernelVersion"},{"sourceId":258470148,"sourceType":"kernelVersion"},{"sourceId":259916067,"sourceType":"kernelVersion"},{"sourceId":260589889,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Dict\nimport asyncio\n\nimport re\nimport pandas as pd\nimport numpy as np\nfrom doraemon import Doraemon\nfrom kg_rag import PromptBuilder, KG_RAG_Tool, Inference\n\n\ndef classify(x: str, n: int = 1200, dataset: str = \"metaqa\") -> pd.DataFrame:\n    BASE_PATHS = {\n        \"metaqa\": \"/kaggle/input/filtered-multiple-hops-metaqa\",\n        \"webqsp\": \"/kaggle/input/webqsp\"\n    }\n\n    # Validate dataset input\n    if dataset not in BASE_PATHS:\n        raise ValueError(f\"Invalid dataset '{dataset}'. Must be one of: {list(BASE_PATHS.keys())}\")\n\n    base = BASE_PATHS[dataset]\n    x = (x or \"\").strip().lower()\n\n    # Select file path based on dataset type\n    if dataset == \"metaqa\":\n        match x:\n            case \"one\":\n                path = f\"{base}/one_hop_supported.pickle\"\n            case \"three\" | _:\n                path = f\"{base}/three_hop_supported.pickle\"\n    else:  # dataset == \"another\"\n        match x:\n            case \"one\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop1_nl.pkl\"\n            case \"three\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop3_nl.pkl\"\n            case \"two\" | _:\n                raise ValueError(\"âŒ 'two_hop_supported.pickle' does not exist in the 'current' dataset.\")\n\n    # Load and return dataframe\n    df = pd.read_pickle(path)\n\n    if dataset ==\"webqsp\":\n        df=df.rename(columns={\n            \"ground_truth\":\"Label\",\n            \"contexts\": \"ctx_topk\"\n        })\n    \n    return df.head(n)\n\n\nclass Local:\n\n    @classmethod\n    def norm(cls, x):\n        \"\"\"\n        Normalise answers for robust comparison:\n        - remove backslashes\n        - strip quotes/brackets/whitespace\n        - lowercase\n        - collapse internal whitespace\n        \"\"\"\n        if x is None or (isinstance(x, float) and pd.isna(x)):\n            return None\n        s = str(x)\n        s = s.replace('\\\\', '')\n        s = s.strip().strip('\\'\"[]{}()')\n        s = re.sub(r'\\s+', ' ', s).lower()\n    \n        return s\n    \n    @classmethod\n    def eval_accuracy(\n            cls,\n            df: pd.DataFrame,\n            pred: str,\n            g_t: str,\n            out_col: str = \"is_correct\"\n        ) -> Dict[str, float]:\n            \"\"\"\n            Evaluate accuracy by checking if prediction is inside the label list.\n    \n            Args:\n                df (pd.DataFrame): DataFrame containing predictions and labels.\n                pred (str): Column name for predictions.\n                g_t (str): Column name for ground-truth labels (list-like).\n                out_col (str): Column name to store correctness flags.\n    \n            Returns:\n                dict: {\"accuracy\": float, \"num_correct\": int, \"total\": int}\n            \"\"\"\n    \n            def _in_labels(p, labels) -> bool:\n                if p is None or (isinstance(p, float) and pd.isna(p)):\n                    return False\n\n                if isinstance(labels, np.ndarray):\n                    labels = labels.tolist()\n                if not isinstance(labels, (list, tuple, set)):\n                    return False\n    \n                norm_p =cls.norm(p)\n                norm_labels = {cls.norm(x) for x in labels if x is not None}\n                return norm_p in norm_labels\n    \n            # Mark correctness per row\n            df[out_col] = df.apply(lambda r: _in_labels(r[pred], r[g_t]), axis=1)\n    \n            total = len(df)\n            num_correct = int(df[out_col].sum()) if total else 0\n            accuracy = (num_correct / total) if total else float(\"nan\")\n    \n            return {\"accuracy\": accuracy, \"num_correct\": num_correct, \"total\": total}\n\nclass StanRAG:\n    SYSTEM_PROMPT: Dict[str, str]={\n        'rag':(\n            \"You are an assistant that guess answers for the questions based on the provided context.\"\n            \"Return the final answer and a confidence score between 0.00 and 1.00\"\n            \"in the format: \\\\boxed{{answer, confidence}}. \"\n            \"where <confidence> reflects how confident you are that no correct answer exists in the context.\"\n        )\n    }\n    \n    @classmethod\n    def build_user_content(cls, q, ctxs):\n        # Ensure ctxs is always a list\n        if ctxs is None:\n            ctxs = []\n        if not isinstance(ctxs, (list, tuple)):\n            ctxs = [ctxs]\n    \n        # Take only the first context if available\n        first_ctx = ctxs[0] if len(ctxs) > 0 else \"\"\n    \n        # Build output string\n        lines = [f\"Question: {q}\"]\n        if first_ctx:\n            lines.append(f\"Context1: {first_ctx}\")\n\n        lines.append(\"Return the final answer in \\\\boxed{{answer, confidence}}\")\n    \n        return \"\\n\".join(lines)","metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from calibration_metrics import CalibrationMetrics\n\ndataset=\"webqsp\"\ndf = classify(\"three\", n = 1200, dataset=dataset)    # From MetaQA dataset\n\ndf[\"query\"] = df.apply(\n    lambda row: [\n        {\"role\": \"system\", \"content\": StanRAG.SYSTEM_PROMPT[\"rag\"]},\n        {\"role\": \"user\", \"content\": StanRAG.build_user_content(row[\"question\"], row[\"ctx_topk\"])},\n    ],\n    axis=1\n)\n\nDoraemon.set_provider('llama3')\nlogger=Doraemon.get_logger(logfile='baselines_rag.log')\ntasks = df.to_dict(orient='records')\n\nstd_a = await Inference.process_batches(tasks, logger, 'query')\ndf['std_a'] = pd.Series(std_a, dtype='object')\n\ndf[[\"std_ans\", \"std_cs\"]] = df[\"std_a\"].apply(\n    lambda x: pd.Series(KG_RAG_Tool.extract_boxed_answer_and_confidence(x))\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(Local.eval_accuracy(df, pred='std_ans', g_t='Label', out_col=\"is_correct_std\"))\n\nstd_summary = CalibrationMetrics.summarize(df, prob_col=\"std_cs\", correct_col=\"is_correct_std\", n_bins=10, norm=\"l2\")\nprint(f'ECE:{std_summary[\"ece\"]}, BS:{std_summary[\"brier\"]}, AUC:{std_summary[\"selective_auc\"]}')\n\ndf.to_pickle('standard_rag.pkl')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}