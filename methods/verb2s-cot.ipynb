{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12901053,"sourceType":"datasetVersion","datasetId":8162483},{"sourceId":12936708,"sourceType":"datasetVersion","datasetId":8185966},{"sourceId":250973825,"sourceType":"kernelVersion"},{"sourceId":259916067,"sourceType":"kernelVersion"},{"sourceId":260589889,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport pandas as pd\nfrom doraemon import Doraemon\nfrom typing import List, Optional, Sequence, Dict, Any\nfrom kg_rag import Inference, KG_RAG_Tool\n\nclass Verb2sCoT:\n    \"\"\"\n    Two-stage prompting for Verb_2s_CoT:\n      - Stage 1 (answer): output = \"<reasoning_text> \\\\boxed{<final answer>}\"\n      - Stage 2 (prob):   output = \"\\\\boxed{<probability>}\"\n\n    Notes:\n    - SYSTEM_PROMPT is a single dict with two keys: 'answer' and 'prob'.\n    - The old parse_final_triplet is removed (stage 1 no longer includes probability).\n    \"\"\"\n\n    # -------- Single SYSTEM_PROMPT with two keys --------\n    SYSTEM_PROMPT: Dict[str, str] = {\n        # =========================\n        # Stage 1: Answer prompt\n        # =========================\n        \"answer\": (\n            \"Provide your best guess for the following question.\\n\"\n            \"First give ONE short sentence explaining the decisive clue(s) (no step-by-step reasoning).\\n\"\n            \"Then on a new line give ONLY the guess with no other words or explanation.\\n\\n\"\n            \"For example:\\n\\n\"\n            \"reasoning\\\\boxed{{guess}}\\n\\n\"\n        ),\n\n        # =========================\n        # Stage 2: Probability prompt\n        # =========================\n        \"prob\": (\n            \"Provide the probability that your guess is correct. \"\n            \"Give ONLY the probability, no other words or explanation.\\n\\n\"\n            \"For example:\\n\\n\"\n            \"\\\\boxed{{0.85}}\\n\"\n        ),\n    }\n\n    # ----------- Format contexts helper -----------\n    @staticmethod\n    def _format_contexts(ctxs: Optional[Sequence[str]]) -> str:\n        if not ctxs:\n            return \"\"\n        return \"\\n\".join(f\"Context{i+1}: {c}\" for i, c in enumerate(ctxs))\n\n\n    # ----------- Stage 1 messages (answer) -----------\n    @classmethod\n    def msgs_stage1(cls, question: str, contexts: Optional[Sequence[str]] = None) -> List[Dict[str, str]]:\n        user_prompt = \"\"\n        if contexts:\n            user_prompt += \"## Provided Contexts\\n\" + cls._format_contexts(contexts) + \"\\n\\n\"\n        user_prompt += f\"Question: {question}\"\n        user_prompt += \"return ONLY like: reasoning\\\\boxed{{guess}}\\n\\n\"\n        return [\n            {\"role\": \"system\", \"content\": cls.SYSTEM_PROMPT[\"answer\"]},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n\n    # ----------- Stage 2 messages (probability) -----------\n    @classmethod\n    def msgs_stage2(\n        cls,\n        final_answer: str,\n        question: str,\n        contexts: Optional[Sequence[str]] = None,\n    ) -> List[Dict[str, str]]:\n        \"\"\"\n        Build OpenAI-style messages for Stage 2 (verbalised probability):\n        - System prompt = SYSTEM_PROMPT[\"prob\"] (unchanged baseline content).\n        - User prompt includes contexts (if any), the original question, and the final answer from Stage 1.\n        \"\"\"\n        user_prompt = \"\"\n        if contexts:\n            user_prompt += \"## Provided Contexts\\n\" + cls._format_contexts(contexts) + \"\\n\\n\"\n        user_prompt += f\"Question: {question}\\n\"\n        user_prompt += f\"Final Answer: {final_answer}\"\n        user_prompt += \"Return ONLY like: \\\\boxed{{0.85}}\\n\"\n    \n        return [\n            {\"role\": \"system\", \"content\": cls.SYSTEM_PROMPT[\"prob\"]},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n\n\n\ndef classify(x: str, n: int = 1200, dataset: str = \"metaqa\") -> pd.DataFrame:\n    BASE_PATHS = {\n        \"metaqa\": \"/kaggle/input/filtered-multiple-hops-metaqa\",\n        \"webqsp\": \"/kaggle/input/webqsp\"\n    }\n\n    # Validate dataset input\n    if dataset not in BASE_PATHS:\n        raise ValueError(f\"Invalid dataset '{dataset}'. Must be one of: {list(BASE_PATHS.keys())}\")\n\n    base = BASE_PATHS[dataset]\n    x = (x or \"\").strip().lower()\n\n    # Select file path based on dataset type\n    if dataset == \"metaqa\":\n        match x:\n            case \"one\":\n                path = f\"{base}/one_hop_supported.pickle\"\n            case \"three\" | _:\n                path = f\"{base}/three_hop_supported.pickle\"\n    else:  # dataset == \"another\"\n        match x:\n            case \"one\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop1_nl.pkl\"\n            case \"three\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop3_nl.pkl\"\n            case \"two\" | _:\n                raise ValueError(\"‚ùå 'two_hop_supported.pickle' does not exist in the 'current' dataset.\")\n\n    # Load and return dataframe\n    df = pd.read_pickle(path)\n\n    if dataset ==\"webqsp\":\n        df=df.rename(columns={\n            \"ground_truth\":\"Label\",\n            \"contexts\": \"ctx_topk\"\n        })\n    \n    return df.head(n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset=\"webqsp\"\n\ndf = classify(\"three\", n = 1200, dataset=dataset)\n\ndf['query']=df.apply(\n    lambda x: Verb2sCoT.msgs_stage1(\n        question=x['question'],\n        contexts=x['ctx_topk']\n    ),\n    axis=1\n)\n\nDoraemon.set_provider('llama3')\nlogger=Doraemon.get_logger(logfile='verb_2s_cot.log')\ntasks = df.to_dict(orient='records')\n\nq_a = await Inference.process_batches(tasks, logger, 'query')\ndf['q_a'] = pd.Series(q_a, dtype='object')\ndf['q_a'] = df['q_a'].apply(KG_RAG_Tool.extract_boxed_answer)","metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['query_prob']=df.apply(\n    lambda x: Verb2sCoT.msgs_stage2(\n        final_answer=x['q_a'],\n        question=x['question'],\n        contexts=x['ctx_topk']\n    ), \n    axis=1\n)\n\ntasks = df.to_dict(orient='records')\n\nquery_prob_a = await Inference.process_batches(tasks, logger, 'query_prob')\ndf['q_a_prob'] = pd.Series(query_prob_a, dtype='object')\n\ndf['final_a']=df['q_a'].copy()\ndf['final_prob']=df['q_a_prob'].apply(KG_RAG_Tool.extract_boxed_answer)\n\nprint(KG_RAG_Tool.eval_accuracy(df, pred='final_a', g_t='Label'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from calibration_metrics import CalibrationMetrics\n\nstd_summary = CalibrationMetrics.summarize(df, prob_col=\"final_prob\", correct_col=\"is_correct\", n_bins=10, norm=\"l2\")\nprint(std_summary[\"ece\"], std_summary[\"brier\"], std_summary[\"selective_auc\"])\ntbl_std = std_summary[\"reliability_table\"]","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_pickle('verb2s_cot.pickle')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}