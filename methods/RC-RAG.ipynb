{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12901053,"sourceType":"datasetVersion","datasetId":8162483},{"sourceId":12936708,"sourceType":"datasetVersion","datasetId":8185966},{"sourceId":250973825,"sourceType":"kernelVersion"},{"sourceId":259916067,"sourceType":"kernelVersion"},{"sourceId":260589889,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom doraemon import Doraemon\nfrom kg_rag import Judgment, Fusion, KG_RAG_Tool, Inference\n\n\nclass PromptBuilder:\n    \"\"\"\n    A class encapsulating the prompt templates used throughout the KG‑RAG\n    pipeline. Two dictionaries are exposed: PROMPT for the initial RAG\n    prompting and PROMPT_FUSION for the fusion stage. Helper methods are\n    provided to assemble user messages.\n    \"\"\"\n    PROMPT = {\n        'rag': (\n            \"Use the provided contexts to answer the fact checking question. \"\n            \"If the contexts are incomplete or weak, still provide your best possible answer. \"\n            \"Always return a confidence score between 0.00 and 1.00 reflecting how confident you are that the final answer is correct. \"\n            \"Output MUST be exactly one line in this format:\\n\"\n            \"\\\\boxed{{final answer, confidence score}}\\n\"\n            \"Do not include any other text. Examples:\\n\"\n            \"\\\\boxed{{answer, 0.92}}\\n\"\n        ),\n        'cf_use': (\n            \"Assume your previous answer is wrong due to improper use of the retrieved contexts. \"\n            \"Carefully re-check the provided contexts and regenerate the answer using one or a few words. \"\n            \"Always return a confidence score between 0.00 and 1.00 reflecting how confident you are that the final answer is correct. \"\n            \"Output MUST be exactly one line in this format:\\n\"\n            \"\\\\boxed{{final answer, confidence score}}\\n\"\n            \"Do not include any other text. Examples:\\n\"\n            \"\\\\boxed{{answer, 0.95}}\\n\"\n        ),\n        'cf_quality': (\n            \"Assume your previous answer is wrong because the quality of the referred contexts is poor. \"\n            \"Re-select the most relevant parts from the given contexts and regenerate the answer using one or a few words. \"\n            \"Always return a confidence score between 0.00 and 1.00 reflecting how confident you are that the final answer is correct. \"\n            \"Output MUST be exactly one line in this format:\\n\"\n            \"\\\\boxed{{final answer, confidence score}}\\n\"\n            \"Do not include any other text. Examples:\\n\"\n            \"\\\\boxed{{answer, 0.88}}\\n\"\n        )\n    }\n\n    @classmethod\n    def build_user_multi_contents(cls, q, ctxs):\n        if ctxs is None:\n            ctxs = []\n        if not isinstance(ctxs, (list, tuple)):\n            ctxs = [ctxs]\n        lines = [f\"Question: {q}\"] + [f\"\\nContext{i+1}: {c}\" for i, c in enumerate(ctxs)]\n        return \"\".join(lines)\n\n\ndef classify(x: str, n: int = 1200, dataset: str = \"metaqa\") -> pd.DataFrame:\n    BASE_PATHS = {\n        \"metaqa\": \"/kaggle/input/filtered-multiple-hops-metaqa\",\n        \"webqsp\": \"/kaggle/input/webqsp\"\n    }\n\n    # Validate dataset input\n    if dataset not in BASE_PATHS:\n        raise ValueError(f\"Invalid dataset '{dataset}'. Must be one of: {list(BASE_PATHS.keys())}\")\n\n    base = BASE_PATHS[dataset]\n    x = (x or \"\").strip().lower()\n\n    # Select file path based on dataset type\n    if dataset == \"metaqa\":\n        match x:\n            case \"one\":\n                path = f\"{base}/one_hop_supported.pickle\"\n            case \"three\" | _:\n                path = f\"{base}/three_hop_supported.pickle\"\n    else:  # dataset == \"another\"\n        match x:\n            case \"one\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop1_nl.pkl\"\n            case \"three\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop3_nl.pkl\"\n            case \"two\" | _:\n                raise ValueError(\"❌ 'two_hop_supported.pickle' does not exist in the 'current' dataset.\")\n\n    # Load and return dataframe\n    df = pd.read_pickle(path)\n\n    if dataset ==\"webqsp\":\n        df=df.rename(columns={\n            \"ground_truth\":\"Label\",\n            \"contexts\": \"ctx_topk\"\n        })\n    \n    return df.head(n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset=\"metaqa\"\n\ndf = classify(\"one\", n = 1, dataset=dataset) \n\ndf['query'] = df.apply(lambda row: [\n    {\"role\": \"system\", \"content\": PromptBuilder.PROMPT['rag']},\n    {\"role\": \"user\", \"content\": PromptBuilder.build_user_multi_contents(row['question'], row['ctx_topk'])}\n], axis=1)\n\ndf['cf_use'] = df.apply(lambda row: [\n    {\"role\": \"system\", \"content\": PromptBuilder.PROMPT['cf_use']},\n    {\"role\": \"user\", \"content\": PromptBuilder.build_user_multi_contents(row['question'], row['ctx_topk'])}\n], axis=1)\n\ndf['cf_quality'] = df.apply(lambda row: [\n    {\"role\": \"system\", \"content\": PromptBuilder.PROMPT['cf_quality']},\n    {\"role\": \"user\", \"content\": PromptBuilder.build_user_multi_contents(row['question'], row['ctx_topk'])}\n], axis=1)\n\nDoraemon.set_provider('gpt3')\nlogger = Doraemon.get_logger(logfile='rkag_emnlp.log')\ntasks = df.to_dict(orient='records')\n\ninit_a = await Inference.process_batches(tasks, logger, 'query')\ndf['init_a'] = pd.Series(init_a, dtype='object')\ncf_use_a = await Inference.process_batches(tasks, logger, 'cf_use')\ndf['cf_use_a'] = pd.Series(cf_use_a, dtype='object')\ncf_quality_a = await Inference.process_batches(tasks, logger, 'cf_quality')\ndf['cf_qual_a'] = pd.Series(cf_quality_a, dtype='object')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = Judgment.apply_judgments(df)\ndf = await Fusion.resolve_f_cases(df, logger)\ndf = await Fusion.compute_probabilities(df, logger)","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import brier_score_loss\nfrom torchmetrics.classification import BinaryCalibrationError\nfrom calibration_metrics import CalibrationMetrics\n\n\ndef selective_auc_trapz(df, prob_col=\"final_P\", correct_col=\"is_correct\"):\n    y = df[correct_col].astype(int).to_numpy()\n    p = pd.to_numeric(df[prob_col], errors=\"coerce\").clip(0, 1).to_numpy()\n    if len(y) == 0:\n        return np.nan, np.array([]), np.array([])\n    # sort by prob desc (ties keep row order; SAUC will depend on that)\n    idx = np.argsort(-p, kind=\"mergesort\")\n    y_sorted = y[idx]\n    n = len(y_sorted)\n    coverage = np.arange(1, n + 1) / n\n    accuracy_curve = np.cumsum(y_sorted) / np.arange(1, n + 1)\n    auc = np.trapz(accuracy_curve, coverage)\n    return float(auc), coverage, accuracy_curve\n\ndef ece_l1(df, prob_col=\"final_P\", correct_col=\"is_correct\", n_bins=10):\n    y = df[correct_col].astype(int).to_numpy()\n    p = pd.to_numeric(df[prob_col], errors=\"coerce\").clip(0, 1).to_numpy()\n    if len(y) == 0:\n        return np.nan\n    bins = np.linspace(0.0, 1.0, n_bins + 1)\n    idx = np.digitize(p, bins, right=True)\n    idx[idx == 0] = 1\n    idx[idx > n_bins] = n_bins\n    ece = 0.0\n    N = len(p)\n    for b in range(1, n_bins + 1):\n        m = (idx == b)\n        if not m.any():\n            continue\n        conf = p[m].mean()\n        acc = y[m].mean()\n        ece += (m.mean()) * abs(acc - conf)\n    return float(ece)\n\ndef brier(df, prob_col=\"final_P\", correct_col=\"is_correct\"):\n    p = pd.to_numeric(df[prob_col], errors=\"coerce\")   # Series\n    y = df[correct_col].astype(\"Int64\")                # Series\n    m = p.notna() & y.notna()\n    if not m.any():\n        return np.nan\n    return float(brier_score_loss(y[m].astype(int).to_numpy(),\n                                  p[m].clip(0, 1).to_numpy()))\n\n\nprint(KG_RAG_Tool.eval_accuracy(df, pred='fusion_prob', g_t='Label'))\nauc, cov, acc = selective_auc_trapz(df, prob_col=\"fusion_prob\", correct_col=\"is_correct\")\n# ece = ece_l1(df, prob_col=\"final_P\", correct_col=\"is_correct\", n_bins=10)\nbs = brier(df, prob_col=\"fusion_prob\", correct_col=\"is_correct\")\n\n# L2\nece_l2 = CalibrationMetrics.ece_torchmetrics_binary(\n    df, prob_col=\"fusion_prob\", correct_col=\"is_correct\", n_bins=10, norm=\"l2\"\n)\n\nprint(\"ECE (L2)\",ece_l2)\nprint(\"Brier score:\", bs)\nprint(\"Selective AUC (trapz):\", auc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_pickle('rkag_emnlp.pkl')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}