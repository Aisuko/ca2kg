{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12901053,"sourceType":"datasetVersion","datasetId":8162483},{"sourceId":12936708,"sourceType":"datasetVersion","datasetId":8185966},{"sourceId":250973825,"sourceType":"kernelVersion"},{"sourceId":259916067,"sourceType":"kernelVersion"},{"sourceId":260589889,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\n\nimport pandas as pd\nfrom kg_rag import Inference, KG_RAG_Tool\nfrom doraemon import Doraemon\nfrom typing import List, Optional, Sequence, Dict\n\nclass Verb1STop:\n\n    SYSTEM_PROMPT = (\n        \"Provide your 4 BEST FINAL ANSWERS and the probability that each is correct (0.0 to 1.0) \"\n        \"for the following question.\\n\\n\"\n        \"STRICT REQUIREMENTS:\\n\"\n        \"1) Output EXACTLY 4 final answers, ordered by DESCENDING probability.\\n\"\n        \"2) Each answer is a short token/phrase (NOT a sentence) and MUST NOT contain the characters ':', ',', '{', '}', '[', ']'.\\n\"\n        \"3) Each probability is a numeric value between 0.0 and 1.0 (e.g., 0.95 or 1.0).\\n\"\n        \"4) Output ONLY the 4 answers and probabilities—no extra words, no explanations.\\n\"\n        \"5) The final output MUST follow this EXACT format, including the outer square brackets and inner braces:\\n\"\n        \"   \\\\boxed{{[{a1:p1},{a2:p2},{a3:p3},{a4:p4}]}}\\n\"\n        \"   - Note: Do NOT omit the leading '[' or the trailing ']'.\\n\"\n        \"   - Note: Each item MUST be wrapped in braces {…}.\\n\"\n        \"\\n\"\n        \"Where:\\n\"\n        \"- a1, a2, a3, a4 = your 4 most likely final answers (no ':', ',', '{', '}', '[', ']').\\n\"\n        \"- p1, p2, p3, p4 = probabilities for each answer.\\n\"\n        \"\\n\"\n        \"Example (format only):\\n\"\n        \"\\\\boxed{{[{True:0.75},{False:0.15},{True:0.07},{False:0.03}]}}\\n\"\n    )\n\n\n    # ----------- Format contexts helper (optional) -----------\n    @staticmethod\n    def _format_contexts(ctxs: Optional[Sequence[str]]) -> str:\n        if not ctxs:\n            return \"\"\n        return \"\\n\".join(f\"Context{i+1}: {c}\" for i, c in enumerate(ctxs))\n\n    # ----------- Build messages (system + user) -----------\n    @classmethod\n    def msgs(cls, question: str, contexts: Optional[Sequence[str]] = None) -> List[Dict[str, str]]:\n        \"\"\"\n        Build OpenAI-style messages:\n        - System prompt contains strict format & final answer rules.\n        - User prompt contains the question + optional contexts.\n        \"\"\"\n        user_prompt_parts = []\n        if contexts:\n            user_prompt_parts.append(\"## Provided Contexts\")\n            user_prompt_parts.append(cls._format_contexts(contexts))\n            user_prompt_parts.append(\"\")  # blank line\n        user_prompt_parts.append(f\"Question: {question}\")\n        user_prompt = \"\\n\".join(user_prompt_parts)\n\n        return [\n            {\"role\": \"system\", \"content\": cls.SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n\n\n    @classmethod\n    def get_first_guess_info(cls, extracted: str):\n        \"\"\"\n        From the extracted string '[a1:p1],[a2:p2],...' or '{a1:p1},{a2:p2},...',\n        return (final_a, probability) from the first valid pair.\n        \"\"\"\n\n        if not extracted or pd.isna(extracted):\n            return None, None\n\n        # Regex to match the first [answer:prob] or {answer:prob} pair\n        match = re.search(r'[\\[\\{]\\s*([^:\\]\\}]+?)\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)\\s*[\\]\\}]', extracted)\n        if not match:\n            return None, None\n\n        # Extract answer and probability safely\n        answer = match.group(1).strip()\n        try:\n            probability = float(match.group(2))\n        except ValueError:\n            probability = None\n\n        return answer, probability\n\n\ndef classify(x: str, n: int = 1200, dataset: str = \"metaqa\") -> pd.DataFrame:\n    BASE_PATHS = {\n        \"metaqa\": \"/kaggle/input/filtered-multiple-hops-metaqa\",\n        \"webqsp\": \"/kaggle/input/webqsp\"\n    }\n\n    # Validate dataset input\n    if dataset not in BASE_PATHS:\n        raise ValueError(f\"Invalid dataset '{dataset}'. Must be one of: {list(BASE_PATHS.keys())}\")\n\n    base = BASE_PATHS[dataset]\n    x = (x or \"\").strip().lower()\n\n    # Select file path based on dataset type\n    if dataset == \"metaqa\":\n        match x:\n            case \"one\":\n                path = f\"{base}/one_hop_supported.pickle\"\n            case \"three\" | _:\n                path = f\"{base}/three_hop_supported.pickle\"\n    else:  # dataset == \"another\"\n        match x:\n            case \"one\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop1_nl.pkl\"\n            case \"three\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop3_nl.pkl\"\n            case \"two\" | _:\n                raise ValueError(\"❌ 'two_hop_supported.pickle' does not exist in the 'current' dataset.\")\n\n    # Load and return dataframe\n    df = pd.read_pickle(path)\n\n    if dataset ==\"webqsp\":\n        df=df.rename(columns={\n            \"ground_truth\":\"Label\",\n            \"contexts\": \"ctx_topk\"\n        })\n    \n    return df.head(n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset=\"webqsp\"\n\ndf = classify(\"three\", n = 1200, dataset=dataset)\ndf['query'] = df.apply(lambda row: Verb1STop.msgs(row['question'], row['ctx_topk']), axis=1)\n\nDoraemon.set_provider('llama3')\nlogger = Doraemon.get_logger(logfile='verb_1s_top_4.log')\ntasks = df.to_dict(orient='records')\nq_a = await Inference.process_batches(tasks, logger, 'query')\ndf['q_a'] = pd.Series(q_a, dtype='object')\n\ndf[['final_a', 'fusion_prob']] = df['q_a'].apply(\n    lambda x: pd.Series(\n        Verb1STop.get_first_guess_info(KG_RAG_Tool.extract_boxed_answer(x))\n    )\n)","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from calibration_metrics import CalibrationMetrics\n\nprint(KG_RAG_Tool.eval_accuracy(df, 'final_a', 'Label'))\n# 1) Use your std_* columns\nstd_summary = CalibrationMetrics.summarize(df, prob_col=\"fusion_prob\", correct_col=\"is_correct\", n_bins=10, norm=\"l2\")\nprint(std_summary[\"ece\"], std_summary[\"brier\"], std_summary[\"selective_auc\"])\ntbl_std = std_summary[\"reliability_table\"]","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_pickle('verb1s_top4.pkl')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}