{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12901053,"sourceType":"datasetVersion","datasetId":8162483},{"sourceId":12936708,"sourceType":"datasetVersion","datasetId":8185966},{"sourceId":250973825,"sourceType":"kernelVersion"},{"sourceId":258470148,"sourceType":"kernelVersion"},{"sourceId":258838986,"sourceType":"kernelVersion"},{"sourceId":259912204,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import List, Dict\n\nimport json\nimport numpy as np\nimport pandas as pd\nfrom doraemon import Doraemon\nfrom kg_rag import Judgment, Fusion, KG_RAG_Tool, PromptBuilder, Inference\n\n\nclass Local:\n    PROMPT = {\n        'rag': (\n            \"Use the provided contexts to answer the question. \"\n            \"If the contexts are incomplete or weak, still provide your best possible answer. \"\n            \"Output MUST be exactly one line in this format:\\n\"\n            \"\\\\boxed{{final answer}}\\n\"\n            \"Do not include any other text. Examples:\\n\"\n            \"\\\\boxed{{Italian}}\\n\"\n        ),\n        'cf_use': (\n            \"Assume your previous answer is wrong due to improper use of the retrieved contexts. \"\n            \"Carefully re-check the provided contexts and regenerate the answer using one or a few words. \"\n            \"Output MUST be exactly one line in this format:\\n\"\n            \"\\\\boxed{{final answer}}\\n\"\n            \"Do not include any other text. Examples:\\n\"\n            \"\\\\boxed{{Italian Languages}}\\n\"\n        ),\n        'cf_quality': (\n            \"Assume your previous answer is wrong because the quality of the referred contexts is poor. \"\n            \"Re-select the most relevant parts from the given contexts and regenerate the answer using one or a few words. \"\n            \"Output MUST be exactly one line in this format:\\n\"\n            \"\\\\boxed{{final answer}}\\n\"\n            \"Do not include any other text. Examples:\\n\"\n            \"\\\\boxed{{Italian Languages}}\\n\"\n        ),\n        \"score\": (\n            \"You are given 3 candidate guesses for the same question from different prompts:\\n\"\n            \" - baseline: G1\\n\"\n            \" - cf_use:   G2\\n\"\n            \" - cf_qual:  G3\\n\"\n            \"\\n\"\n            \"Task:\\n\"\n            \"1) Build a unified set of UNIQUE answers by merging semantically identical strings across G1, G2, G3.\\n\"\n            \"2) Compute a GLOBAL frequency vector over the unique answers based on how many of G1/G2/G3 map to each canonical answer.\\n\"\n            \"3) ASSIGN probabilities for EACH scenario (baseline, cf_use, cf_qual) to be EXACTLY this global frequency vector normalized by 3 (counts/3), after duplicate aggregation and before rounding.\\n\"\n            \"\\n\"\n            \"Merging rules:\\n\"\n            \"- Ignore case, whitespace, punctuation, and plural/singular differences.\\n\"\n            \"- Choose a clean canonical form (e.g., title case).\\n\"\n            \"- If multiple inputs (G1/G2/G3) map to the SAME canonical answer, AGGREGATE by SUMMING that answer's frequency before normalization.\\n\"\n            \"\\n\"\n            \"Probability rules (HARD CONSTRAINTS):\\n\"\n            \"A) Let count[i] be how many of {G1,G2,G3} map to answers[i]. Then for every scenario S in {baseline, cf_use, cf_qual}, set S[i] = round(count[i]/3, 2).\\n\"\n            \"B) Do NOT output equal probabilities across answers when counts differ.\\n\"\n            \"C) Each probability ∈ [0.00, 1.00]; sums may be < 1.00 after rounding.\\n\"\n            \"D) If a scenario has no plausible answers, use zeros.\\n\"\n            \"\\n\"\n            \"Return EXACTLY one line of STRICT JSON, no wrapper, no extra text:\\n\"\n            \"{\"\n            \"  \\\"answers\\\": [\\\"answer1\\\",\\\"answer2\\\",...],\"\n            \"  \\\"baseline\\\": [p1,p2,...],\"\n            \"  \\\"cf_use\\\": [p1,p2,...],\"\n            \"  \\\"cf_qual\\\": [p1,p2,...]\"\n            \"}\\n\"\n            \"Example (inputs):\\n\"\n            \"G1(baseline): Masti\\n\"\n            \"G2(cf_use):   Masti Returns\\n\"\n            \"G3(cf_qual):  Masti\\n\"\n            \"\\n\"\n            \"Valid output (counts: Masti=2, Masti Returns=1 → probs=[0.88 0.12] for ALL scenarios):\\n\"\n            \"{\"\n            \"  \\\"answers\\\": [\\\"Masti\\\",\\\"Masti Returns\\\"],\"\n            \"  \\\"baseline\\\": [0.88 0.12],\"\n            \"  \\\"cf_use\\\": [0.88 0.12],\"\n            \"  \\\"cf_qual\\\": [0.88 0.12]\"\n            \"}\"\n        )\n    }\n\n\n    @classmethod\n    def causal_f(\n        cls,\n        candidates: str,          # e.g., \"a1,a2,a3,a4\"\n        q: str,\n        ctxs: str,\n        system_msg: str,\n        k: int = 3\n    ) -> List[Dict[str, str]]:\n        \"\"\"\n        Build chat messages for Stage 2 (probability scoring).\n        `candidates` is a single comma-separated string like \"a1,a2,a3,a4\".\n        Output must be: \\boxed{p1,p2,...,pk}\n        \"\"\"\n        # Minimal safeguard: trim whitespace\n        cand_str = (candidates or \"\").strip()\n    \n        user_msg = (\n            f\"QUESTION:\\n{q}\\n\\n\"\n            f\"CONTEXT:\\n{ctxs}\\n\\n\"\n            \"YOUR GUESSES (comma-separated, in order):\\n\"\n            f\"{cand_str}\\n\\n\"\n            \"Example (inputs come from other prompts):\\n\"\n            \"G1: Masti\\n\"\n            \"G2: Masti Returns\\n\"\n            \"G3: masti\\n\"\n        )\n    \n        return [\n            {\"role\": \"system\", \"content\": system_msg},\n            {\"role\": \"user\", \"content\": user_msg},\n        ]\n\ndef classify(x: str, n: int = 1200, dataset: str = \"metaqa\") -> pd.DataFrame:\n    BASE_PATHS = {\n        \"metaqa\": \"/kaggle/input/filtered-multiple-hops-metaqa\",\n        \"webqsp\": \"/kaggle/input/webqsp\"\n    }\n\n    # Validate dataset input\n    if dataset not in BASE_PATHS:\n        raise ValueError(f\"Invalid dataset '{dataset}'. Must be one of: {list(BASE_PATHS.keys())}\")\n\n    base = BASE_PATHS[dataset]\n    x = (x or \"\").strip().lower()\n\n    # Select file path based on dataset type\n    if dataset == \"metaqa\":\n        match x:\n            case \"one\":\n                path = f\"{base}/one_hop_supported.pickle\"\n            case \"three\" | _:\n                path = f\"{base}/three_hop_supported.pickle\"\n    else:  # dataset == \"another\"\n        match x:\n            case \"one\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop1_nl.pkl\"\n            case \"three\":\n                path = f\"{base}/webqsp_ctxstyle_1200_hop3_nl.pkl\"\n            case \"two\" | _:\n                raise ValueError(\"❌ 'two_hop_supported.pickle' does not exist in the 'current' dataset.\")\n\n    # Load and return dataframe\n    df = pd.read_pickle(path)\n\n    if dataset ==\"webqsp\":\n        df=df.rename(columns={\n            \"ground_truth\":\"Label\",\n            \"contexts\": \"ctx_topk\"\n        })\n    \n    return df.head(n)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-09-04T07:42:14.682919Z","iopub.execute_input":"2025-09-04T07:42:14.683924Z","iopub.status.idle":"2025-09-04T07:42:14.697723Z","shell.execute_reply.started":"2025-09-04T07:42:14.683887Z","shell.execute_reply":"2025-09-04T07:42:14.696779Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset=\"webqsp\"\n\ndf = classify(\"three\", n = 1200, dataset=dataset)\n\ndf['query'] = df.apply(lambda row: [\n    {\"role\": \"system\", \"content\": Local.PROMPT['rag']},\n    {\"role\": \"user\", \"content\": PromptBuilder.build_user_multi_contents(row['question'], row['ctx_topk'])}\n], axis=1)\n\nlogger = Doraemon.get_logger(logfile='rkag.log')\ntasks = df.to_dict(orient='records')\n\ninit_a = await Inference.process_batches(tasks, logger, 'query')\ndf['init_a'] = pd.Series(init_a, dtype='object')\ndf['init_a'] = df['init_a'].apply(KG_RAG_Tool.extract_boxed_answer)\n\ndf['cf_use'] = df.apply(lambda row: PromptBuilder.alter_answer(row['question'], row['ctx_topk'], row['init_a'], Local.PROMPT['cf_use']), axis=1)\n\ntasks = df.to_dict(orient='records')\ncf_use_a = await Inference.process_batches(tasks, logger, 'cf_use')\ndf['cf_use_a'] = pd.Series(cf_use_a, dtype='object')\ndf['cf_use_a'] = df['cf_use_a'].apply(KG_RAG_Tool.extract_boxed_answer)\n\ndf['cf_qual'] = df.apply(lambda row: PromptBuilder.alter_answer(row['question'], row['ctx_topk'], row['init_a'], Local.PROMPT['cf_quality']), axis=1)\n\ntasks = df.to_dict(orient='records')\ncf_qual_a = await Inference.process_batches(tasks, logger, 'cf_qual')\ndf['cf_qual_a'] = pd.Series(cf_qual_a, dtype='object')\ndf['cf_qual_a'] = df['cf_qual_a'].apply(KG_RAG_Tool.extract_boxed_answer)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-09-04T07:42:14.699254Z","iopub.execute_input":"2025-09-04T07:42:14.699561Z","iopub.status.idle":"2025-09-04T07:43:51.146883Z","shell.execute_reply.started":"2025-09-04T07:42:14.699535Z","shell.execute_reply":"2025-09-04T07:43:51.145998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_final_answer_prob(json_str):\n    data = json.loads(json_str)\n    answers = data[\"answers\"]\n    baseline, cf_use, cf_qual = data[\"baseline\"], data[\"cf_use\"], data[\"cf_qual\"]\n\n    final_scores = {}\n    for i, ans in enumerate(answers):\n        vals = [baseline[i], cf_use[i], cf_qual[i]]\n        ce = max(vals) - min(vals)\n        c = sum(vals) / 3.0   # mean, always divide by 3\n        final_scores[ans] = c * (1 - ce)\n\n    # pick the answer with the largest P_stable\n    final_answer = max(final_scores, key=final_scores.get)\n    return final_answer, round(final_scores[final_answer], 4)\n\n\ndf['causal_f'] = df.apply(lambda row: Local.causal_f(\n    candidates=f\"G1:{row['init_a']},G2:{row['cf_use_a']},G3:{row['cf_qual_a']}\",\n    q=row['question'],\n    ctxs=row['ctx_topk'],\n    system_msg=Local.PROMPT['score']\n), axis=1)\n\ntasks = df.to_dict(orient='records')\ncausal_f_a = await Inference.process_batches(tasks, logger, 'causal_f')\ndf['causal_f_a'] = pd.Series(causal_f_a, dtype='object')\n\n\nbad_rows = []\n\ndef safe_get_final_answer_prob(idx, x):\n    try:\n        return pd.Series(get_final_answer_prob(x[0]))\n    except Exception:\n        bad_rows.append(idx)  # record original row index\n        return pd.Series([None, None])\n\n# Apply with row context so we can capture the index\ndf[[\"final_a\", \"final_P\"]] = df.apply(\n    lambda row: safe_get_final_answer_prob(row.name, row[\"causal_f_a\"]),\n    axis=1\n)\n\n# DataFrame of bad rows from the original df\ndf_bad = df.loc[bad_rows].reset_index().rename(columns={\"index\": \"orig_index\"})\ndf_bad.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:43:51.148015Z","iopub.execute_input":"2025-09-04T07:43:51.148311Z","iopub.status.idle":"2025-09-04T07:44:25.307132Z","shell.execute_reply.started":"2025-09-04T07:43:51.148289Z","shell.execute_reply":"2025-09-04T07:44:25.306187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import brier_score_loss\nfrom torchmetrics.classification import BinaryCalibrationError\nfrom calibration_metrics import CalibrationMetrics\n\n\ndef selective_auc_trapz(df, prob_col=\"final_P\", correct_col=\"is_correct\"):\n    y = df[correct_col].astype(int).to_numpy()\n    p = pd.to_numeric(df[prob_col], errors=\"coerce\").clip(0, 1).to_numpy()\n    if len(y) == 0:\n        return np.nan, np.array([]), np.array([])\n    # sort by prob desc (ties keep row order; SAUC will depend on that)\n    idx = np.argsort(-p, kind=\"mergesort\")\n    y_sorted = y[idx]\n    n = len(y_sorted)\n    coverage = np.arange(1, n + 1) / n\n    accuracy_curve = np.cumsum(y_sorted) / np.arange(1, n + 1)\n    auc = np.trapz(accuracy_curve, coverage)\n    return float(auc), coverage, accuracy_curve\n\ndef ece_l1(df, prob_col=\"final_P\", correct_col=\"is_correct\", n_bins=10):\n    y = df[correct_col].astype(int).to_numpy()\n    p = pd.to_numeric(df[prob_col], errors=\"coerce\").clip(0, 1).to_numpy()\n    if len(y) == 0:\n        return np.nan\n    bins = np.linspace(0.0, 1.0, n_bins + 1)\n    idx = np.digitize(p, bins, right=True)\n    idx[idx == 0] = 1\n    idx[idx > n_bins] = n_bins\n    ece = 0.0\n    N = len(p)\n    for b in range(1, n_bins + 1):\n        m = (idx == b)\n        if not m.any():\n            continue\n        conf = p[m].mean()\n        acc = y[m].mean()\n        ece += (m.mean()) * abs(acc - conf)\n    return float(ece)\n\ndef brier(df, prob_col=\"final_P\", correct_col=\"is_correct\"):\n    p = pd.to_numeric(df[prob_col], errors=\"coerce\")   # Series\n    y = df[correct_col].astype(\"Int64\")                # Series\n    m = p.notna() & y.notna()\n    if not m.any():\n        return np.nan\n    return float(brier_score_loss(y[m].astype(int).to_numpy(),\n                                  p[m].clip(0, 1).to_numpy()))\n\n\nprint(KG_RAG_Tool.eval_accuracy(df, pred='fusion_prob', g_t='Label'))\nauc, cov, acc = selective_auc_trapz(df, prob_col=\"fusion_prob\", correct_col=\"is_correct\")\n# ece = ece_l1(df, prob_col=\"final_P\", correct_col=\"is_correct\", n_bins=10)\nbs = brier(df, prob_col=\"fusion_prob\", correct_col=\"is_correct\")\n\n# L2\nece_l2 = CalibrationMetrics.ece_torchmetrics_binary(\n    df, prob_col=\"fusion_prob\", correct_col=\"is_correct\", n_bins=10, norm=\"l2\"\n)\n\nprint(\"ECE (L2)\",ece_l2)\nprint(\"Brier score:\", bs)\nprint(\"Selective AUC (trapz):\", auc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:44:25.308212Z","iopub.execute_input":"2025-09-04T07:44:25.308578Z","iopub.status.idle":"2025-09-04T07:44:25.334787Z","shell.execute_reply.started":"2025-09-04T07:44:25.308546Z","shell.execute_reply":"2025-09-04T07:44:25.333722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_pickle(\"rkag_m.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}